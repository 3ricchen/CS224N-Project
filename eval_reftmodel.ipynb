{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from my_datasets import (\n",
    "  ParaphraseDetectionDataset,\n",
    "  ParaphraseDetectionTestDataset,\n",
    "  load_paraphrase_data\n",
    ")\n",
    "from evaluation_reft import model_eval_paraphrase, model_test_paraphrase\n",
    "from models.gpt2 import GPT2Model\n",
    "\n",
    "from optimizer import AdamW\n",
    "import transformers\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "import os\n",
    "import pyreft\n",
    "\n",
    "import pyvene as pv\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollator,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from datasets import Dataset\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional, Sequence\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers.utils import logging\n",
    "from transformers.trainer_utils import (\n",
    "    EvalPrediction,\n",
    "    has_length,\n",
    "    denumpify_detensorize\n",
    ")\n",
    "from pyreft import ReftDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervention key: layer_19_comp_block_output_unit_pos_nunit_1#0\n",
      "Intervention key: layer_24_comp_block_output_unit_pos_nunit_1#0\n",
      "Intervention key: layer_29_comp_block_output_unit_pos_nunit_1#0\n",
      "Intervention key: layer_35_comp_block_output_unit_pos_nunit_1#0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The key is provided in the config. Assuming this is loaded from a pretrained module.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervention key: layer_19_comp_block_output_unit_pos_nunit_1#0\n",
      "Intervention key: layer_24_comp_block_output_unit_pos_nunit_1#0\n",
      "Intervention key: layer_29_comp_block_output_unit_pos_nunit_1#0\n",
      "Intervention key: layer_35_comp_block_output_unit_pos_nunit_1#0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The key is provided in the config. Assuming this is loaded from a pretrained module.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpt2 = transformers.AutoModelForCausalLM.from_pretrained('gpt2-large').to(device)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2-large')\n",
    "gpt2_tokenizer.padding_side = \"left\"\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "EOS_TOKEN=gpt2_tokenizer.eos_token\n",
    "\n",
    "model_name_or_path = \"gpt2-large\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "\n",
    "reft_model = pyreft.ReftModel.load(\n",
    "    load_directory = \"./reft_gpt_large_PARAPHRASE_BIGGER_AND_BETTER\", model = model, from_huggingface_hub = False\n",
    ")\n",
    "reft_model.set_device(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding side: left\n"
     ]
    }
   ],
   "source": [
    "print('Padding side:', gpt2_tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 283003 train examples from data/quora-train.csv\n",
      "Loaded 40429 train examples from data/quora-dev.csv\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    para_train=\"data/quora-train.csv\",\n",
    "    para_dev=\"data/quora-dev.csv\",\n",
    "    para_test=\"data/quora-test-student.csv\",\n",
    "    para_dev_out=\"predictions/para-dev-output.csv\",\n",
    "    para_test_out=\"predictions/para-test-output.csv\",\n",
    "    seed=11711,\n",
    "    epochs=10,\n",
    "    use_gpu=False,  # change to True if you want GPU usage\n",
    "    batch_size=32,\n",
    "    lr=1e-5,\n",
    "    model_size=\"gpt2-large\"\n",
    ")\n",
    "\n",
    "para_train_data = load_paraphrase_data(args.para_train)[:1000]\n",
    "para_dev_data = load_paraphrase_data(args.para_dev)[:1000]\n",
    "\n",
    "para_train_data = ParaphraseDetectionDataset(para_train_data, args, tokenizer = gpt2_tokenizer)\n",
    "para_dev_data = ParaphraseDetectionDataset(para_dev_data, args, tokenizer = gpt2_tokenizer)\n",
    "\n",
    "para_train_dataloader = DataLoader(para_train_data, shuffle=True, batch_size=args.batch_size,\n",
    "                                    collate_fn=para_train_data.collate_fn)\n",
    "para_dev_dataloader = DataLoader(para_dev_data, shuffle=False, batch_size=128,\n",
    "                                collate_fn=para_dev_data.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def model_eval_paraphrase_intervenable(dataloader, model, device, tokenizer, TQDM_DISABLE = False):\n",
    "    model.eval()  # Turn off dropout and other randomness.\n",
    "    y_true, y_pred, sent_ids = [], [], []\n",
    "    \n",
    "    yes_token_id = tokenizer.encode(\"yes\", add_special_tokens=False)[0]\n",
    "    # step, batch = next(enumerate(tqdm(dataloader, desc='eval', disable=TQDM_DISABLE)))\n",
    "    # Decode batch with tokenizer\n",
    "    # base_unit_location = batch[\"input_ids\"].shape[-1] - 1\n",
    "    # print(batch)\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc='eval', disable=TQDM_DISABLE)):\n",
    "        b_ids = batch['token_ids'].to(device)\n",
    "        b_mask = batch['attention_mask'].to(device)\n",
    "        b_sent_ids = batch['sent_ids']\n",
    "        labels = batch['labels'].flatten()\n",
    "        \n",
    "        \n",
    "        # Compute the actual length (number of non-padded tokens) for each example.\n",
    "        # Assuming that b_mask contains 1s for tokens and 0s for padding.\n",
    "        lengths = b_mask.sum(dim=1)  # shape: [batch_size]\n",
    "        \n",
    "        # For each example, the base unit is the last non-padded token.\n",
    "        # Create a nested list (one per sample) in the expected format.\n",
    "        total_length = b_ids.shape[1]\n",
    "        unit_locations_batch = [[[total_length - 1]] for _ in range(b_ids.shape[0])]\n",
    "        \n",
    "        # Prepare the input dictionary for the base prompt.\n",
    "        prompt_batch = {\"input_ids\": b_ids, \"attention_mask\": b_mask}\n",
    "\n",
    "        _, reft_response = model.generate(\n",
    "            prompt_batch,\n",
    "            unit_locations={\"sources->base\": (None, unit_locations_batch)},\n",
    "            intervene_on_prompt=True,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True, \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Decode the generated response.\n",
    "        decoded_response = tokenizer.batch_decode(reft_response, skip_special_tokens=True)\n",
    "        # print(decoded_response)\n",
    "        \n",
    "        \n",
    "        # break\n",
    "\n",
    "        # Process each generated output.\n",
    "        first_generated = reft_response[:, total_length]\n",
    "        pred_batch = (first_generated == yes_token_id).long()\n",
    "        true_batch = (labels.cpu() == yes_token_id).long()\n",
    "        \n",
    "        y_pred.extend(pred_batch.cpu().numpy().tolist())\n",
    "        y_true.extend(true_batch.numpy().tolist())\n",
    "        sent_ids.extend(b_sent_ids)\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    return acc, f1, y_pred, y_true, sent_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_tokens = gpt2_tokenizer.encode(\"<|assistant|>:\")\n",
    "print(\"Expected tokens for '<|assistant|>:'\", expected_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, f1, y_pred, y_true, sent_ids = model_eval_paraphrase_intervenable(para_dev_dataloader, reft_model, 'cuda', gpt2_tokenizer, TQDM_DISABLE = False)\n",
    "print('Dev Accuracy:', acc)\n",
    "print('Dev F1:', f1)\n",
    "print('y1 sum:', sum(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lfs/skampere1/0/ericc27/miniconda3/envs/cs224n/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:677: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:   3%|▎         | 1/32 [00:00<00:06,  5.12it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:   6%|▋         | 2/32 [00:00<00:06,  4.62it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:   9%|▉         | 3/32 [00:00<00:07,  3.70it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  12%|█▎        | 4/32 [00:00<00:06,  4.01it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  16%|█▌        | 5/32 [00:01<00:06,  3.99it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  19%|█▉        | 6/32 [00:01<00:06,  3.95it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  22%|██▏       | 7/32 [00:01<00:06,  4.15it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  25%|██▌       | 8/32 [00:02<00:06,  3.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  28%|██▊       | 9/32 [00:02<00:06,  3.53it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  31%|███▏      | 10/32 [00:02<00:05,  3.71it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  34%|███▍      | 11/32 [00:02<00:05,  3.54it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  38%|███▊      | 12/32 [00:03<00:05,  3.48it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  41%|████      | 13/32 [00:03<00:05,  3.66it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  44%|████▍     | 14/32 [00:03<00:04,  3.73it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  47%|████▋     | 15/32 [00:03<00:04,  3.92it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  50%|█████     | 16/32 [00:04<00:03,  4.24it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  53%|█████▎    | 17/32 [00:04<00:03,  4.53it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  56%|█████▋    | 18/32 [00:04<00:02,  4.80it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  59%|█████▉    | 19/32 [00:04<00:02,  5.01it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  62%|██████▎   | 20/32 [00:04<00:02,  5.21it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  66%|██████▌   | 21/32 [00:05<00:02,  5.01it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  69%|██████▉   | 22/32 [00:05<00:01,  5.20it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  72%|███████▏  | 23/32 [00:05<00:01,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  75%|███████▌  | 24/32 [00:05<00:01,  5.44it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  78%|███████▊  | 25/32 [00:05<00:01,  5.41it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  81%|████████▏ | 26/32 [00:05<00:01,  5.45it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  84%|████████▍ | 27/32 [00:06<00:00,  5.27it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  88%|████████▊ | 28/32 [00:06<00:00,  5.20it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  91%|█████████ | 29/32 [00:06<00:00,  5.20it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  94%|█████████▍| 30/32 [00:06<00:00,  5.25it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval:  97%|█████████▋| 31/32 [00:06<00:00,  5.07it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "eval: 100%|██████████| 32/32 [00:07<00:00,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.55\n",
      "Train F1: 0.4964888489063738\n",
      "y1 sum: 274\n",
      "True sum 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "acc, f1, y_pred, y_true, sent_ids = model_eval_paraphrase_intervenable(para_train_dataloader, reft_model, 'cuda', gpt2_tokenizer, TQDM_DISABLE = False)\n",
    "print('Train Accuracy:', acc)\n",
    "print('Train F1:', f1)\n",
    "print('y1 sum:', sum(y_pred))\n",
    "print('True sum', sum(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_collator(tokenizer, model) -> ReftDataCollator:\n",
    "    data_collator_fn = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        padding=\"longest\",\n",
    "        max_length=2048,\n",
    "    )\n",
    "    return ReftDataCollator(data_collator=data_collator_fn)\n",
    "\n",
    "\n",
    "def make_dataloader(dataset: Dataset, batch_size: int, collate_fn: DataCollatorForSeq2Seq, shuffle: bool) -> DataLoader:\n",
    "    return DataLoader(dataset, shuffle=shuffle, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "def extract_answer(generation, trigger_tokens=\"\"):\n",
    "    \"\"\"\n",
    "    Extract the predicted answer (assumed to be the first token after any trigger text).\n",
    "    \"\"\"\n",
    "    if trigger_tokens and trigger_tokens in generation:\n",
    "        generation = generation.split(trigger_tokens)[-1]\n",
    "    # Take the first token as the answer.\n",
    "    answer = generation.strip().split()[0].lower()\n",
    "    # Ensure the answer is either \"yes\" or \"no\"\n",
    "    if answer not in [\"yes\", \"no\"]:\n",
    "        if answer.startswith(\"y\"):\n",
    "            answer = \"yes\"\n",
    "        elif answer.startswith(\"n\"):\n",
    "            answer = \"no\"\n",
    "    return answer\n",
    "\n",
    "def evaluate_paraphrase_detection(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_dataset,\n",
    "    batch_size=4,\n",
    "    generation_args=None,\n",
    "    data_collator = None,\n",
    "    device=None,\n",
    "    trigger_tokens=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned GPT2-Large model for paraphrase detection using generation.\n",
    "    This function sets up the intervenable generation call by including:\n",
    "      - the \"base\" argument with input_ids and attention_mask,\n",
    "      - \"unit_locations\" computed from intervention locations (if provided) or a dummy value,\n",
    "      - intervention on prompt enabled.\n",
    "    It then decodes the outputs, extracts the answer, and computes accuracy.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(eval_dataset[0])\n",
    "    data_collator = data_collator if data_collator is not None else \\\n",
    "        make_data_collator(tokenizer, model.model)\n",
    "\n",
    "    dataloader = make_dataloader(eval_dataset, batch_size, data_collator, shuffle=False)\n",
    "    print(next(iter(dataloader)))\n",
    "    return\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    predictions = []\n",
    "    gold_labels = []\n",
    "\n",
    "    # Set default generation parameters; these can be updated via generation_args.\n",
    "    default_generation_args = {\n",
    "        \"max_length\": 512,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"do_sample\": False,  # Greedy decoding by default\n",
    "        \"num_beams\": 1       # Default to greedy decoding\n",
    "    }\n",
    "    if generation_args:\n",
    "        default_generation_args.update(generation_args)\n",
    "    eval_iterator = tqdm(dataloader, position=0, leave=True)\n",
    "    for steps, inputs in enumerate(eval_iterator):\n",
    "        print(inputs)\n",
    "        # Move inputs to device.\n",
    "        input_ids = batch[\"token_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        batch_labels_ids = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Compute intervention locations following the sample compute_metrics function.\n",
    "        # If your collate_fn provides \"intervention_locations\", use them;\n",
    "        # otherwise, use a dummy value.\n",
    "        if \"intervention_locations\" in batch:\n",
    "            intervention_locations = batch[\"intervention_locations\"]\n",
    "            if intervention_locations.dim() == 3:\n",
    "                intervention_locations = intervention_locations.permute(1, 0, 2)\n",
    "            # Adjust intervention locations by computing left padding from the BOS token.\n",
    "            left_padding = (batch[\"token_ids\"] == tokenizer.bos_token_id).nonzero(as_tuple=True)[1]\n",
    "            if left_padding.numel() > 0:\n",
    "                left_padding = left_padding.reshape(1, -1, 1).to(device)\n",
    "                intervention_locations = intervention_locations + left_padding\n",
    "                intervention_locations = intervention_locations - 1  # Offset for sink padding.\n",
    "            else:\n",
    "                print(\"Warning: No BOS token found, skipping left padding adjustment.\")\n",
    "            # If using beam search, repeat intervention locations accordingly.\n",
    "            num_beams = default_generation_args.get(\"num_beams\", 1)\n",
    "            if num_beams > 1:\n",
    "                intervention_locations = intervention_locations.repeat_interleave(num_beams, dim=1).tolist()\n",
    "            else:\n",
    "                intervention_locations = intervention_locations.tolist()\n",
    "        else:\n",
    "            print('ALERT ALERT')\n",
    "            intervention_locations = 0  # Dummy intervention_locations for non-intervenable batches.\n",
    "\n",
    "        # Construct the generation arguments including the \"base\" input and intervention locations.\n",
    "        gen_args = {\n",
    "            \"base\": {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            \"unit_locations\": {\"sources->base\": (None, intervention_locations)},\n",
    "            \"intervene_on_prompt\": True,\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "            \"early_stopping\": True,\n",
    "            \"max_length\": default_generation_args.get(\"max_length\", 50),\n",
    "            \"pad_token_id\": tokenizer.pad_token_id,\n",
    "            \"do_sample\": default_generation_args.get(\"do_sample\", False),\n",
    "        }\n",
    "        # Optionally add extra generation parameters.\n",
    "        for key in [\"temperature\", \"top_p\", \"top_k\", \"num_beams\"]:\n",
    "            if key in default_generation_args:\n",
    "                gen_args[key] = default_generation_args[key]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generation_output = model.generate(**gen_args)\n",
    "            if isinstance(generation_output, (tuple, list)):\n",
    "                outputs = generation_output[1]\n",
    "            else:\n",
    "                outputs = generation_output\n",
    "\n",
    "\n",
    "        # Decode generated outputs and gold labels.\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(batch_labels_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Process each output: extract the predicted answer and compare with gold.\n",
    "        for pred_text, label_text in zip(decoded_outputs, decoded_labels):\n",
    "            pred_answer = extract_answer(pred_text, trigger_tokens)\n",
    "            label_answer = extract_answer(label_text, trigger_tokens)\n",
    "            predictions.append(pred_answer)\n",
    "            gold_labels.append(label_answer)\n",
    "            if pred_answer == label_answer:\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0.0\n",
    "    print(f\"Paraphrase Detection Accuracy: {accuracy:.3f}\")\n",
    "    return predictions, gold_labels, accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
