{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from my_datasets import (\n",
    "  ParaphraseDetectionDataset,\n",
    "  ParaphraseDetectionTestDataset,\n",
    "  load_paraphrase_data\n",
    ")\n",
    "from evaluation_reft import model_eval_paraphrase, model_test_paraphrase\n",
    "from models.gpt2 import GPT2Model\n",
    "\n",
    "from optimizer import AdamW\n",
    "import transformers\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "import os\n",
    "import pyreft\n",
    "\n",
    "import pyvene as pv\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollator,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from datasets import Dataset\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional, Sequence\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers.utils import logging\n",
    "from transformers.trainer_utils import (\n",
    "    EvalPrediction,\n",
    "    has_length,\n",
    "    denumpify_detensorize\n",
    ")\n",
    "from pyreft import ReftDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervention key: layer_19_comp_block_output_unit_pos_nunit_1#0\n",
      "Intervention key: layer_24_comp_block_output_unit_pos_nunit_1#0\n",
      "Intervention key: layer_29_comp_block_output_unit_pos_nunit_1#0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The key is provided in the config. Assuming this is loaded from a pretrained module.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervention key: layer_35_comp_block_output_unit_pos_nunit_1#0\n",
      "Intervention key: layer_19_comp_block_output_unit_pos_nunit_1#0\n",
      "Intervention key: layer_24_comp_block_output_unit_pos_nunit_1#0\n",
      "Intervention key: layer_29_comp_block_output_unit_pos_nunit_1#0\n",
      "Intervention key: layer_35_comp_block_output_unit_pos_nunit_1#0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The key is provided in the config. Assuming this is loaded from a pretrained module.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpt2 = transformers.AutoModelForCausalLM.from_pretrained('gpt2-large').to(device)\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "# gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2-large')\n",
    "gpt2_tokenizer.padding_side = \"left\"\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "EOS_TOKEN=gpt2_tokenizer.eos_token\n",
    "\n",
    "model_name_or_path = \"gpt2-large\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "\n",
    "reft_model = pyreft.ReftModel.load(\n",
    "    load_directory = \"./reft_gpt_large_PARAPHRASE_BIGGER_AND_BETTER\", model = model, from_huggingface_hub = False\n",
    ")\n",
    "reft_model.set_device(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding side: left\n"
     ]
    }
   ],
   "source": [
    "print('Padding side:', gpt2_tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 283003 train examples from data/quora-train.csv\n",
      "Loaded 40429 train examples from data/quora-dev.csv\n"
     ]
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    para_train=\"data/quora-train.csv\",\n",
    "    para_dev=\"data/quora-dev.csv\",\n",
    "    para_test=\"data/quora-test-student.csv\",\n",
    "    para_dev_out=\"predictions/para-dev-output.csv\",\n",
    "    para_test_out=\"predictions/para-test-output.csv\",\n",
    "    seed=11711,\n",
    "    epochs=10,\n",
    "    use_gpu=False,  # change to True if you want GPU usage\n",
    "    batch_size=32,\n",
    "    lr=1e-5,\n",
    "    model_size=\"gpt2-large\"\n",
    ")\n",
    "\n",
    "para_train_data = load_paraphrase_data(args.para_train)\n",
    "para_dev_data = load_paraphrase_data(args.para_dev)\n",
    "\n",
    "para_train_data = ParaphraseDetectionDataset(para_train_data, args, tokenizer = gpt2_tokenizer)\n",
    "para_dev_data = ParaphraseDetectionDataset(para_dev_data, args, tokenizer = gpt2_tokenizer)\n",
    "\n",
    "para_train_dataloader = DataLoader(para_train_data, shuffle=True, batch_size=args.batch_size,\n",
    "                                    collate_fn=para_train_data.collate_fn)\n",
    "para_dev_dataloader = DataLoader(para_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                collate_fn=para_dev_data.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def model_eval_paraphrase_intervenable(dataloader, model, device, tokenizer, TQDM_DISABLE = False):\n",
    "    model.eval()  # Turn off dropout and other randomness.\n",
    "    y_true, y_pred, sent_ids = [], [], []\n",
    "    \n",
    "    yes_token_id = tokenizer.encode(\"yes\", add_special_tokens=False)[0]\n",
    "    # step, batch = next(enumerate(tqdm(dataloader, desc='eval', disable=TQDM_DISABLE)))\n",
    "    # Decode batch with tokenizer\n",
    "    # base_unit_location = batch[\"input_ids\"].shape[-1] - 1\n",
    "    # print(batch)\n",
    "    for step, batch in enumerate(tqdm(dataloader, desc='eval', disable=TQDM_DISABLE)):\n",
    "        b_ids = batch['token_ids'].to(device)\n",
    "        b_mask = batch['attention_mask'].to(device)\n",
    "        b_sent_ids = batch['sent_ids']\n",
    "        labels = batch['labels'].flatten()\n",
    "        \n",
    "        \n",
    "        # Compute the actual length (number of non-padded tokens) for each example.\n",
    "        # Assuming that b_mask contains 1s for tokens and 0s for padding.\n",
    "        lengths = b_mask.sum(dim=1)  # shape: [batch_size]\n",
    "        \n",
    "        # For each example, the base unit is the last non-padded token.\n",
    "        # Create a nested list (one per sample) in the expected format.\n",
    "        total_length = b_ids.shape[1]\n",
    "        unit_locations_batch = [[[total_length - 1]] for _ in range(b_ids.shape[0])]\n",
    "        \n",
    "        # Prepare the input dictionary for the base prompt.\n",
    "        prompt_batch = {\"input_ids\": b_ids, \"attention_mask\": b_mask}\n",
    "\n",
    "        _, reft_response = model.generate(\n",
    "            prompt_batch,\n",
    "            unit_locations={\"sources->base\": (None, unit_locations_batch)},\n",
    "            intervene_on_prompt=True,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=True, \n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Process each generated output.\n",
    "        first_generated = reft_response[:, total_length]\n",
    "        pred_batch = (first_generated == yes_token_id).long()\n",
    "        true_batch = (labels.cpu() == yes_token_id).long()\n",
    "        \n",
    "        y_pred.extend(pred_batch.cpu().numpy().tolist())\n",
    "        y_true.extend(true_batch.numpy().tolist())\n",
    "        sent_ids.extend(b_sent_ids)\n",
    "        break\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    return acc, f1, y_pred, y_true, sent_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_collator(tokenizer, model) -> ReftDataCollator:\n",
    "    data_collator_fn = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100,\n",
    "        padding=\"longest\",\n",
    "        max_length=2048,\n",
    "    )\n",
    "    return ReftDataCollator(data_collator=data_collator_fn)\n",
    "\n",
    "\n",
    "def make_dataloader(dataset: Dataset, batch_size: int, collate_fn: DataCollatorForSeq2Seq, shuffle: bool) -> DataLoader:\n",
    "    return DataLoader(dataset, shuffle=shuffle, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "def extract_answer(generation, trigger_tokens=\"\"):\n",
    "    \"\"\"\n",
    "    Extract the predicted answer (assumed to be the first token after any trigger text).\n",
    "    \"\"\"\n",
    "    if trigger_tokens and trigger_tokens in generation:\n",
    "        generation = generation.split(trigger_tokens)[-1]\n",
    "    # Take the first token as the answer.\n",
    "    answer = generation.strip().split()[0].lower()\n",
    "    # Ensure the answer is either \"yes\" or \"no\"\n",
    "    if answer not in [\"yes\", \"no\"]:\n",
    "        if answer.startswith(\"y\"):\n",
    "            answer = \"yes\"\n",
    "        elif answer.startswith(\"n\"):\n",
    "            answer = \"no\"\n",
    "    return answer\n",
    "\n",
    "def evaluate_paraphrase_detection(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_dataset,\n",
    "    batch_size=4,\n",
    "    generation_args=None,\n",
    "    data_collator = None,\n",
    "    device=None,\n",
    "    trigger_tokens=\"\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a fine-tuned GPT2-Large model for paraphrase detection using generation.\n",
    "    This function sets up the intervenable generation call by including:\n",
    "      - the \"base\" argument with input_ids and attention_mask,\n",
    "      - \"unit_locations\" computed from intervention locations (if provided) or a dummy value,\n",
    "      - intervention on prompt enabled.\n",
    "    It then decodes the outputs, extracts the answer, and computes accuracy.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(eval_dataset[0])\n",
    "    data_collator = data_collator if data_collator is not None else \\\n",
    "        make_data_collator(tokenizer, model.model)\n",
    "\n",
    "    dataloader = make_dataloader(eval_dataset, batch_size, data_collator, shuffle=False)\n",
    "    print(next(iter(dataloader)))\n",
    "    return\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    predictions = []\n",
    "    gold_labels = []\n",
    "\n",
    "    # Set default generation parameters; these can be updated via generation_args.\n",
    "    default_generation_args = {\n",
    "        \"max_length\": 512,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"do_sample\": False,  # Greedy decoding by default\n",
    "        \"num_beams\": 1       # Default to greedy decoding\n",
    "    }\n",
    "    if generation_args:\n",
    "        default_generation_args.update(generation_args)\n",
    "    eval_iterator = tqdm(dataloader, position=0, leave=True)\n",
    "    for steps, inputs in enumerate(eval_iterator):\n",
    "        print(inputs)\n",
    "        # Move inputs to device.\n",
    "        input_ids = batch[\"token_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        batch_labels_ids = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Compute intervention locations following the sample compute_metrics function.\n",
    "        # If your collate_fn provides \"intervention_locations\", use them;\n",
    "        # otherwise, use a dummy value.\n",
    "        if \"intervention_locations\" in batch:\n",
    "            intervention_locations = batch[\"intervention_locations\"]\n",
    "            if intervention_locations.dim() == 3:\n",
    "                intervention_locations = intervention_locations.permute(1, 0, 2)\n",
    "            # Adjust intervention locations by computing left padding from the BOS token.\n",
    "            left_padding = (batch[\"token_ids\"] == tokenizer.bos_token_id).nonzero(as_tuple=True)[1]\n",
    "            if left_padding.numel() > 0:\n",
    "                left_padding = left_padding.reshape(1, -1, 1).to(device)\n",
    "                intervention_locations = intervention_locations + left_padding\n",
    "                intervention_locations = intervention_locations - 1  # Offset for sink padding.\n",
    "            else:\n",
    "                print(\"Warning: No BOS token found, skipping left padding adjustment.\")\n",
    "            # If using beam search, repeat intervention locations accordingly.\n",
    "            num_beams = default_generation_args.get(\"num_beams\", 1)\n",
    "            if num_beams > 1:\n",
    "                intervention_locations = intervention_locations.repeat_interleave(num_beams, dim=1).tolist()\n",
    "            else:\n",
    "                intervention_locations = intervention_locations.tolist()\n",
    "        else:\n",
    "            print('ALERT ALERT')\n",
    "            intervention_locations = 0  # Dummy intervention_locations for non-intervenable batches.\n",
    "\n",
    "        # Construct the generation arguments including the \"base\" input and intervention locations.\n",
    "        gen_args = {\n",
    "            \"base\": {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            \"unit_locations\": {\"sources->base\": (None, intervention_locations)},\n",
    "            \"intervene_on_prompt\": True,\n",
    "            \"eos_token_id\": tokenizer.eos_token_id,\n",
    "            \"early_stopping\": True,\n",
    "            \"max_length\": default_generation_args.get(\"max_length\", 50),\n",
    "            \"pad_token_id\": tokenizer.pad_token_id,\n",
    "            \"do_sample\": default_generation_args.get(\"do_sample\", False),\n",
    "        }\n",
    "        # Optionally add extra generation parameters.\n",
    "        for key in [\"temperature\", \"top_p\", \"top_k\", \"num_beams\"]:\n",
    "            if key in default_generation_args:\n",
    "                gen_args[key] = default_generation_args[key]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generation_output = model.generate(**gen_args)\n",
    "            if isinstance(generation_output, (tuple, list)):\n",
    "                outputs = generation_output[1]\n",
    "            else:\n",
    "                outputs = generation_output\n",
    "\n",
    "\n",
    "        # Decode generated outputs and gold labels.\n",
    "        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(batch_labels_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Process each output: extract the predicted answer and compare with gold.\n",
    "        for pred_text, label_text in zip(decoded_outputs, decoded_labels):\n",
    "            pred_answer = extract_answer(pred_text, trigger_tokens)\n",
    "            label_answer = extract_answer(label_text, trigger_tokens)\n",
    "            predictions.append(pred_answer)\n",
    "            gold_labels.append(label_answer)\n",
    "            if pred_answer == label_answer:\n",
    "                correct_count += 1\n",
    "            total_count += 1\n",
    "\n",
    "    accuracy = correct_count / total_count if total_count > 0 else 0.0\n",
    "    print(f\"Paraphrase Detection Accuracy: {accuracy:.3f}\")\n",
    "    return predictions, gold_labels, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected tokens for '<|assistant|>:' [27, 91, 562, 10167, 91, 31175]\n"
     ]
    }
   ],
   "source": [
    "expected_tokens = gpt2_tokenizer.encode(\"<|assistant|>:\")\n",
    "print(\"Expected tokens for '<|assistant|>:'\", expected_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_paraphrase_detection(\n",
    "    model = reft_model,\n",
    "    tokenizer = gpt2_tokenizer,\n",
    "    eval_dataset = para_dev_data,\n",
    "    batch_size = 4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval:   0%|          | 0/1264 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel_eval_paraphrase_intervenable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpara_dev_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreft_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpt2_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTQDM_DISABLE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mmodel_eval_paraphrase_intervenable\u001b[0;34m(dataloader, model, device, tokenizer, TQDM_DISABLE)\u001b[0m\n\u001b[1;32m      6\u001b[0m yes_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# step, batch = next(enumerate(tqdm(dataloader, desc='eval', disable=TQDM_DISABLE)))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Decode batch with tokenizer\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# base_unit_location = batch[\"input_ids\"].shape[-1] - 1\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# print(batch)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m, disable\u001b[38;5;241m=\u001b[39mTQDM_DISABLE)):\n\u001b[1;32m     12\u001b[0m     b_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m     b_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224n/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224n/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224n/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224n/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dfs/scratch0/ericc27/cs224n_project/my_datasets.py:48\u001b[0m, in \u001b[0;36mParaphraseDetectionDataset.collate_fn\u001b[0;34m(self, all_data)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollate_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, all_data):\n\u001b[0;32m---> 48\u001b[0m   sent1 \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_data]\n\u001b[1;32m     49\u001b[0m   sent2 \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_data]\n\u001b[1;32m     50\u001b[0m   \u001b[38;5;66;03m# labels = torch.LongTensor([x[2] for x in all_data])\u001b[39;00m\n",
      "File \u001b[0;32m/dfs/scratch0/ericc27/cs224n_project/my_datasets.py:48\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcollate_fn\u001b[39m(\u001b[38;5;28mself\u001b[39m, all_data):\n\u001b[0;32m---> 48\u001b[0m   sent1 \u001b[38;5;241m=\u001b[39m [\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_data]\n\u001b[1;32m     49\u001b[0m   sent2 \u001b[38;5;241m=\u001b[39m [x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_data]\n\u001b[1;32m     50\u001b[0m   \u001b[38;5;66;03m# labels = torch.LongTensor([x[2] for x in all_data])\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "model_eval_paraphrase_intervenable(para_dev_dataloader, reft_model, 'cuda', gpt2_tokenizer, TQDM_DISABLE = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
