{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from my_datasets import (\n",
    "  ParaphraseDetectionDataset,\n",
    "  ParaphraseDetectionTestDataset,\n",
    "  load_paraphrase_data\n",
    ")\n",
    "from evaluation_reft import model_eval_paraphrase, model_test_paraphrase\n",
    "from models.gpt2 import GPT2Model\n",
    "\n",
    "from optimizer import AdamW\n",
    "import transformers\n",
    "\n",
    "import pyreft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpt2 = transformers.AutoModelForCausalLM.from_pretrained('gpt2-large').to(device)\n",
    "gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2-large', device = 'cuda')\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.unk_token\n",
    "EOS_TOKEN=gpt2_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervention key: layer_34_comp_block_output_unit_pos_nunit_1#0\n",
      "trainable intervention params: 10,244 || trainable model params: 0\n",
      "model params: 774,030,080 || trainable%: 0.0013234627780873839\n"
     ]
    }
   ],
   "source": [
    "## VANILLA REFT CONFIG\n",
    "\n",
    "reft_config = pyreft.ReftConfig(representations={\n",
    "    \"layer\": 34, \"component\": \"block_output\",\n",
    "    # alternatively, you can specify as string component access,\n",
    "    # \"component\": \"model.layers[0].output\",\n",
    "    \"low_rank_dimension\": 128,\n",
    "    \"intervention\": pyreft.LoreftIntervention(embed_dim=gpt2.config.hidden_size,\n",
    "    low_rank_dimension=128)})\n",
    "reft_model = pyreft.get_reft_model(gpt2, reft_config)\n",
    "reft_model.set_device(\"cuda\")\n",
    "reft_model = reft_model.float()\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REFT MULTIPLE INTERVENTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LORA PEFT CONFIG\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=32, lora_alpha=32, target_modules=[\"c_proj\"], layers_to_transform=[33],\n",
    "    use_rslora=True, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(gpt2, peft_config)\n",
    "\n",
    "reft_config = pyreft.ReftConfig(representations=[{\n",
    "    # string component access is enforced for customized model such as a peft model!\n",
    "    \"layer\": l, \"component\": f\"base_model.model.model.layers[{l}].output\",\n",
    "    \"low_rank_dimension\": 32,\n",
    "    \"intervention\": pyreft.LoreftIntervention(embed_dim=model.config.hidden_size,\n",
    "    low_rank_dimension=4)} for l in [33]])\n",
    "\n",
    "reft_model = pyreft.get_reft_model(model, reft_config)\n",
    "# you need to call this to re-enable lora grads!\n",
    "reft_model.model.enable_adapter_layers()\n",
    "reft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    para_train=\"data/quora-train.csv\",\n",
    "    para_dev=\"data/quora-dev.csv\",\n",
    "    para_test=\"data/quora-test-student.csv\",\n",
    "    para_dev_out=\"predictions/para-dev-output.csv\",\n",
    "    para_test_out=\"predictions/para-test-output.csv\",\n",
    "    seed=11711,\n",
    "    epochs=10,\n",
    "    use_gpu=True,  # change to True if you want GPU usage\n",
    "    batch_size=32,\n",
    "    lr=1e-5,\n",
    "    model_size=\"gpt2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 283003 train examples from data/quora-train.csv\n",
      "Loaded 40429 train examples from data/quora-dev.csv\n"
     ]
    }
   ],
   "source": [
    "para_train_data = load_paraphrase_data(args.para_train)\n",
    "para_dev_data = load_paraphrase_data(args.para_dev)\n",
    "\n",
    "para_train_data = ParaphraseDetectionDataset(para_train_data, args, tokenizer = gpt2_tokenizer)\n",
    "para_dev_data = ParaphraseDetectionDataset(para_dev_data, args, tokenizer = gpt2_tokenizer)\n",
    "\n",
    "para_train_dataloader = DataLoader(para_train_data, shuffle=True, batch_size=args.batch_size,\n",
    "                                    collate_fn=para_train_data.collate_fn)\n",
    "para_dev_dataloader = DataLoader(para_dev_data, shuffle=False, batch_size=args.batch_size,\n",
    "                                collate_fn=para_dev_data.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA LOADED\n"
     ]
    }
   ],
   "source": [
    "inputs = [f'<|user|>:Tell me if these questions are asking the same thing.\\nQuestion 1: {p[0]}\\nQuestion 2: {p[1]}\\nAre these questions asking the same thing?</s>\\n<|assistant|>:' for p in para_train_data]\n",
    "outputs = [('yes' if p[2] == 1 else 'no') for p in para_train_data]\n",
    "print('DATA LOADED')\n",
    "data_module = pyreft.make_last_position_supervised_data_module(\n",
    "    gpt2_tokenizer, gpt2, inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/23992/ipykernel_2187680/3232451456.py:11: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `ReftTrainerForCausalLM.__init__`. Use `processing_class` instead.\n",
      "  trainer = pyreft.ReftTrainerForCausalLM(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1932' max='28301' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1932/28301 04:21 < 59:36, 7.37 it/s, Epoch 0.07/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.956500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.354900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.448300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.678600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.239500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.909500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.868900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.777400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.776000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.751300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.766100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.745600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.751800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.737500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Directory ./tmp/checkpoint-500/intervenable_model already exists and contains files. Skipping save to prevent overwriting existing model.\n",
      "Directory ./tmp/checkpoint-1000/intervenable_model already exists and contains files. Skipping save to prevent overwriting existing model.\n",
      "Directory ./tmp/checkpoint-1500/intervenable_model already exists and contains files. Skipping save to prevent overwriting existing model.\n"
     ]
    }
   ],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    num_train_epochs=1, output_dir=\"./tmp\", per_device_train_batch_size=10, \n",
    "    learning_rate=5e-5, logging_steps=100,\n",
    "    lr_scheduler_type=transformers.SchedulerType.LINEAR,\n",
    "    report_to = [], # disable logging\n",
    "    warmup_steps=500,\n",
    "    weight_decay = 0.001\n",
    "    # L2 regularization\n",
    "    # weight_decay=0.01\n",
    "    ) \n",
    "trainer = pyreft.ReftTrainerForCausalLM(\n",
    "    model=reft_model, tokenizer=gpt2_tokenizer, args=training_args, **data_module)\n",
    "_ = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|user|>:Tell me if these questions are asking the same thing.\\nQuestion 1: how do i start learning about artificial intelligence ?\\nQuestion 2: how do you learn artificial intelligence ?\\nAre these questions asking the same thing?</s>\\n<|assistant|>:', '<|user|>:Tell me if these questions are asking the same thing.\\nQuestion 1: how can someone learn biochemistry using first principles thinking ?\\nQuestion 2: how can someone learn neuroscience using first principles thinking ?\\nAre these questions asking the same thing?</s>\\n<|assistant|>:', '<|user|>:Tell me if these questions are asking the same thing.\\nQuestion 1: how do i view a private broadcast on periscope ?\\nQuestion 2: who is alexander khan on periscope ?\\nAre these questions asking the same thing?</s>\\n<|assistant|>:', '<|user|>:Tell me if these questions are asking the same thing.\\nQuestion 1: what are the best companies for android developer in chennai ?\\nQuestion 2: is there any best android app development company in hyderabad ?\\nAre these questions asking the same thing?</s>\\n<|assistant|>:', \"<|user|>:Tell me if these questions are asking the same thing.\\nQuestion 1: what should i do to get selected in gsoc 2018 ?\\nQuestion 2: i 'm a rookie . how should i start preparing for getting selected in gsoc 17 ?\\nAre these questions asking the same thing?</s>\\n<|assistant|>:\"]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(EOS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory './reft_gpt_large_PARAPHRASE_BIGGER' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "save_dir = \"./reft_gpt_large_PARAPHRASE_BIGGER\"\n",
    "if os.path.exists(save_dir):\n",
    "    shutil.rmtree(save_dir)  # Remove the existing directory\n",
    "\n",
    "reft_model.set_device(\"cpu\")  # Move model to CPU before saving\n",
    "reft_model.save(save_directory=save_dir)\n",
    "\n",
    "# reft_model.set_device(\"cpu\") # send back to cpu before saving.\n",
    "# reft_model.save(\n",
    "#     save_directory=\"./reft_gpt_large_PARAPHRASE\", \n",
    "#     overwrite=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The key is provided in the config. Assuming this is loaded from a pretrained module.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intervention key: layer_34_comp_block_output_unit_pos_nunit_1#0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([4, 1280]) from checkpoint, the shape in current model is torch.Size([128, 1280]).\n\tsize mismatch for bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m model_name_or_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2-large\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mAutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      6\u001b[0m     model_name_or_path, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16, device_map\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m----> 8\u001b[0m reft_model \u001b[38;5;241m=\u001b[39m \u001b[43mpyreft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReftModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./reft_gpt_large_PARAPHRASE_BIGGER\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m reft_model\u001b[38;5;241m.\u001b[39mset_device(device)\n\u001b[1;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224n/lib/python3.10/site-packages/pyreft/reft_model.py:26\u001b[0m, in \u001b[0;36mReftModel.load\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 26\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mpv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIntervenableModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ReftModel\u001b[38;5;241m.\u001b[39m_convert_to_reft_model(model)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224n/lib/python3.10/site-packages/pyvene/models/intervenable_base.py:1328\u001b[0m, in \u001b[0;36mIntervenableModel.load\u001b[0;34m(load_directory, model, local_directory, from_huggingface_hub, include_model)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(intervention, TrainableIntervention):\n\u001b[1;32m   1327\u001b[0m         saved_state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(load_directory, binary_filename))\n\u001b[0;32m-> 1328\u001b[0m         \u001b[43mintervention\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;66;03m# load model's trainable parameters as well\u001b[39;00m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_model:\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224n/lib/python3.10/site-packages/pyreft/interventions.py:69\u001b[0m, in \u001b[0;36mLoreftIntervention.load_state_dict\u001b[0;34m(self, state_dict, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, state_dict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     66\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    Overwrite for data-efficiency.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearned_source\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# Caveat: without creating a new layer, it might not work (still not sure why)\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# We have to recreate a layer, and load back the columns.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     overload_w \u001b[38;5;241m=\u001b[39m state_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrotate_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearned_source\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/cs224n/lib/python3.10/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Linear:\n\tsize mismatch for weight: copying a param with shape torch.Size([4, 1280]) from checkpoint, the shape in current model is torch.Size([128, 1280]).\n\tsize mismatch for bias: copying a param with shape torch.Size([4]) from checkpoint, the shape in current model is torch.Size([128])."
     ]
    }
   ],
   "source": [
    "import torch, transformers, pyreft\n",
    "device = \"cuda\"\n",
    "\n",
    "model_name_or_path = \"gpt2-large\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, torch_dtype=torch.bfloat16, device_map=device)\n",
    "\n",
    "reft_model = pyreft.ReftModel.load(\n",
    "    \"./reft_gpt_large_PARAPHRASE_BIGGER\", model\n",
    ")\n",
    "reft_model.set_device(device)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpt2 = transformers.AutoModelForCausalLM.from_pretrained('gpt2-large').to(device)\n",
    "gpt2_tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2-large', device = 'cuda')\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.unk_token\n",
    "EOS_TOKEN=gpt2_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lfs/skampere1/0/ericc27/miniconda3/envs/cs224n/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:677: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>:Tell me if these questions are asking the same thing.\n",
      "Question 1: Are you gay?\n",
      "Question 2: What is the capital of France?\n",
      "Are these questions asking the same thing?</s>\n",
      "<|assistant|>:\n",
      "<|user|>:Tell me if these questions are asking the same thing.\n",
      "Question 1: Are you gay?\n",
      "Question 2: What is the capital of France?\n",
      "Are these questions asking the same thing?</s>\n",
      "<|assistant|>:no\n"
     ]
    }
   ],
   "source": [
    "prompt = [\"<|user|>:Tell me if these questions are asking the same thing.\\nQuestion 1: Are you gay?\\nQuestion 2: What is the capital of France?\\nAre these questions asking the same thing?</s>\\n<|assistant|>:\"]\n",
    "prompt = gpt2_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "full_prompt = gpt2_tokenizer.decode(prompt[\"input_ids\"][0], skip_special_tokens=True)\n",
    "print(full_prompt)\n",
    "base_unit_location = prompt[\"input_ids\"].shape[-1] - 1  # last position\n",
    "_, reft_response = reft_model.generate(\n",
    "    prompt, unit_locations={\"sources->base\": (None, [[[base_unit_location]]])},\n",
    "    intervene_on_prompt=True, max_new_tokens=512, do_sample=True, \n",
    "    eos_token_id=gpt2_tokenizer.eos_token_id, early_stopping=True\n",
    ")\n",
    "print(gpt2_tokenizer.decode(reft_response[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'para_dev_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluation_reft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m model_eval_paraphrase_intervenable\n\u001b[0;32m----> 2\u001b[0m model_eval_paraphrase_intervenable(\u001b[43mpara_dev_dataloader\u001b[49m, reft_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, gpt2_tokenizer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'para_dev_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "from evaluation_reft import model_eval_paraphrase_intervenable\n",
    "model_eval_paraphrase_intervenable(para_dev_dataloader, reft_model, 'cuda', gpt2_tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
