{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.2367054167697256,
  "eval_steps": 500,
  "global_step": 35000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0035334440479135015,
      "grad_norm": 6.695558547973633,
      "learning_rate": 1e-05,
      "loss": 6.9588,
      "step": 100
    },
    {
      "epoch": 0.007066888095827003,
      "grad_norm": 17.40101432800293,
      "learning_rate": 2e-05,
      "loss": 6.4454,
      "step": 200
    },
    {
      "epoch": 0.010600332143740504,
      "grad_norm": 13.768712043762207,
      "learning_rate": 3e-05,
      "loss": 4.4657,
      "step": 300
    },
    {
      "epoch": 0.014133776191654006,
      "grad_norm": 8.135238647460938,
      "learning_rate": 4e-05,
      "loss": 3.4425,
      "step": 400
    },
    {
      "epoch": 0.017667220239567506,
      "grad_norm": 5.051792144775391,
      "learning_rate": 5e-05,
      "loss": 3.2682,
      "step": 500
    },
    {
      "epoch": 0.02120066428748101,
      "grad_norm": 4.2828755378723145,
      "learning_rate": 4.996454026452963e-05,
      "loss": 2.5355,
      "step": 600
    },
    {
      "epoch": 0.02473410833539451,
      "grad_norm": 6.988894939422607,
      "learning_rate": 4.992908052905926e-05,
      "loss": 1.7264,
      "step": 700
    },
    {
      "epoch": 0.02826755238330801,
      "grad_norm": 6.160583972930908,
      "learning_rate": 4.989362079358888e-05,
      "loss": 1.2502,
      "step": 800
    },
    {
      "epoch": 0.031800996431221515,
      "grad_norm": 5.121310710906982,
      "learning_rate": 4.9858161058118505e-05,
      "loss": 1.0281,
      "step": 900
    },
    {
      "epoch": 0.03533444047913501,
      "grad_norm": 2.6608903408050537,
      "learning_rate": 4.9822701322648134e-05,
      "loss": 0.9162,
      "step": 1000
    },
    {
      "epoch": 0.038867884527048514,
      "grad_norm": 23.59239959716797,
      "learning_rate": 4.978724158717776e-05,
      "loss": 0.8675,
      "step": 1100
    },
    {
      "epoch": 0.04240132857496202,
      "grad_norm": 3.4724411964416504,
      "learning_rate": 4.975178185170739e-05,
      "loss": 0.825,
      "step": 1200
    },
    {
      "epoch": 0.045934772622875514,
      "grad_norm": 6.463014125823975,
      "learning_rate": 4.9716322116237014e-05,
      "loss": 0.7879,
      "step": 1300
    },
    {
      "epoch": 0.04946821667078902,
      "grad_norm": 6.510672569274902,
      "learning_rate": 4.968086238076664e-05,
      "loss": 0.7764,
      "step": 1400
    },
    {
      "epoch": 0.05300166071870252,
      "grad_norm": 2.7093605995178223,
      "learning_rate": 4.9645402645296265e-05,
      "loss": 0.7516,
      "step": 1500
    },
    {
      "epoch": 0.05653510476661602,
      "grad_norm": 19.72724151611328,
      "learning_rate": 4.9609942909825894e-05,
      "loss": 0.7574,
      "step": 1600
    },
    {
      "epoch": 0.06006854881452952,
      "grad_norm": 47.90681457519531,
      "learning_rate": 4.957448317435552e-05,
      "loss": 0.7388,
      "step": 1700
    },
    {
      "epoch": 0.06360199286244303,
      "grad_norm": 29.00130271911621,
      "learning_rate": 4.9539023438885145e-05,
      "loss": 0.7544,
      "step": 1800
    },
    {
      "epoch": 0.06713543691035652,
      "grad_norm": 3.3493988513946533,
      "learning_rate": 4.9503563703414774e-05,
      "loss": 0.7375,
      "step": 1900
    },
    {
      "epoch": 0.07066888095827002,
      "grad_norm": 33.95429611206055,
      "learning_rate": 4.94681039679444e-05,
      "loss": 0.739,
      "step": 2000
    },
    {
      "epoch": 0.07420232500618353,
      "grad_norm": 11.333407402038574,
      "learning_rate": 4.943264423247403e-05,
      "loss": 0.7435,
      "step": 2100
    },
    {
      "epoch": 0.07773576905409703,
      "grad_norm": 11.34502124786377,
      "learning_rate": 4.9397184497003654e-05,
      "loss": 0.7182,
      "step": 2200
    },
    {
      "epoch": 0.08126921310201053,
      "grad_norm": 15.205278396606445,
      "learning_rate": 4.9361724761533277e-05,
      "loss": 0.7222,
      "step": 2300
    },
    {
      "epoch": 0.08480265714992404,
      "grad_norm": 27.072185516357422,
      "learning_rate": 4.9326265026062906e-05,
      "loss": 0.7271,
      "step": 2400
    },
    {
      "epoch": 0.08833610119783754,
      "grad_norm": 3.0699973106384277,
      "learning_rate": 4.9290805290592535e-05,
      "loss": 0.724,
      "step": 2500
    },
    {
      "epoch": 0.09186954524575103,
      "grad_norm": 4.41155481338501,
      "learning_rate": 4.9255345555122164e-05,
      "loss": 0.7345,
      "step": 2600
    },
    {
      "epoch": 0.09540298929366453,
      "grad_norm": 14.274317741394043,
      "learning_rate": 4.921988581965179e-05,
      "loss": 0.7173,
      "step": 2700
    },
    {
      "epoch": 0.09893643334157803,
      "grad_norm": 5.800712585449219,
      "learning_rate": 4.9184426084181415e-05,
      "loss": 0.7311,
      "step": 2800
    },
    {
      "epoch": 0.10246987738949154,
      "grad_norm": 7.204211711883545,
      "learning_rate": 4.914896634871104e-05,
      "loss": 0.7243,
      "step": 2900
    },
    {
      "epoch": 0.10600332143740504,
      "grad_norm": 15.465134620666504,
      "learning_rate": 4.9113506613240666e-05,
      "loss": 0.7233,
      "step": 3000
    },
    {
      "epoch": 0.10953676548531854,
      "grad_norm": 15.076364517211914,
      "learning_rate": 4.9078046877770295e-05,
      "loss": 0.7145,
      "step": 3100
    },
    {
      "epoch": 0.11307020953323205,
      "grad_norm": 6.388981342315674,
      "learning_rate": 4.9042587142299924e-05,
      "loss": 0.7256,
      "step": 3200
    },
    {
      "epoch": 0.11660365358114554,
      "grad_norm": 25.280540466308594,
      "learning_rate": 4.9007127406829546e-05,
      "loss": 0.7035,
      "step": 3300
    },
    {
      "epoch": 0.12013709762905904,
      "grad_norm": 8.16844367980957,
      "learning_rate": 4.8971667671359175e-05,
      "loss": 0.7031,
      "step": 3400
    },
    {
      "epoch": 0.12367054167697254,
      "grad_norm": 12.649064064025879,
      "learning_rate": 4.89362079358888e-05,
      "loss": 0.7128,
      "step": 3500
    },
    {
      "epoch": 0.12720398572488606,
      "grad_norm": 17.707204818725586,
      "learning_rate": 4.8900748200418426e-05,
      "loss": 0.6917,
      "step": 3600
    },
    {
      "epoch": 0.13073742977279953,
      "grad_norm": 11.216161727905273,
      "learning_rate": 4.8865288464948055e-05,
      "loss": 0.7043,
      "step": 3700
    },
    {
      "epoch": 0.13427087382071304,
      "grad_norm": 19.301464080810547,
      "learning_rate": 4.882982872947768e-05,
      "loss": 0.6997,
      "step": 3800
    },
    {
      "epoch": 0.13780431786862654,
      "grad_norm": 21.27545738220215,
      "learning_rate": 4.8794368994007306e-05,
      "loss": 0.708,
      "step": 3900
    },
    {
      "epoch": 0.14133776191654004,
      "grad_norm": 7.4705491065979,
      "learning_rate": 4.8758909258536935e-05,
      "loss": 0.6938,
      "step": 4000
    },
    {
      "epoch": 0.14487120596445355,
      "grad_norm": 10.233155250549316,
      "learning_rate": 4.8723449523066564e-05,
      "loss": 0.71,
      "step": 4100
    },
    {
      "epoch": 0.14840465001236705,
      "grad_norm": 17.960216522216797,
      "learning_rate": 4.8687989787596187e-05,
      "loss": 0.7029,
      "step": 4200
    },
    {
      "epoch": 0.15193809406028055,
      "grad_norm": 4.921745777130127,
      "learning_rate": 4.865253005212581e-05,
      "loss": 0.6899,
      "step": 4300
    },
    {
      "epoch": 0.15547153810819406,
      "grad_norm": 18.67436981201172,
      "learning_rate": 4.861707031665544e-05,
      "loss": 0.6986,
      "step": 4400
    },
    {
      "epoch": 0.15900498215610756,
      "grad_norm": 12.341383934020996,
      "learning_rate": 4.858161058118507e-05,
      "loss": 0.6934,
      "step": 4500
    },
    {
      "epoch": 0.16253842620402106,
      "grad_norm": 7.118551254272461,
      "learning_rate": 4.8546150845714696e-05,
      "loss": 0.7014,
      "step": 4600
    },
    {
      "epoch": 0.16607187025193457,
      "grad_norm": 7.111945629119873,
      "learning_rate": 4.8510691110244325e-05,
      "loss": 0.6801,
      "step": 4700
    },
    {
      "epoch": 0.16960531429984807,
      "grad_norm": 19.385887145996094,
      "learning_rate": 4.847523137477395e-05,
      "loss": 0.6959,
      "step": 4800
    },
    {
      "epoch": 0.17313875834776157,
      "grad_norm": 10.201252937316895,
      "learning_rate": 4.843977163930357e-05,
      "loss": 0.6871,
      "step": 4900
    },
    {
      "epoch": 0.17667220239567508,
      "grad_norm": 3.269280433654785,
      "learning_rate": 4.84043119038332e-05,
      "loss": 0.6901,
      "step": 5000
    },
    {
      "epoch": 0.18020564644358858,
      "grad_norm": 8.320474624633789,
      "learning_rate": 4.836885216836283e-05,
      "loss": 0.7072,
      "step": 5100
    },
    {
      "epoch": 0.18373909049150206,
      "grad_norm": 14.569536209106445,
      "learning_rate": 4.8333392432892456e-05,
      "loss": 0.6778,
      "step": 5200
    },
    {
      "epoch": 0.18727253453941556,
      "grad_norm": 13.590063095092773,
      "learning_rate": 4.829793269742208e-05,
      "loss": 0.6946,
      "step": 5300
    },
    {
      "epoch": 0.19080597858732906,
      "grad_norm": 23.149097442626953,
      "learning_rate": 4.826247296195171e-05,
      "loss": 0.6817,
      "step": 5400
    },
    {
      "epoch": 0.19433942263524256,
      "grad_norm": 20.009849548339844,
      "learning_rate": 4.822701322648133e-05,
      "loss": 0.6847,
      "step": 5500
    },
    {
      "epoch": 0.19787286668315607,
      "grad_norm": 1.8288291692733765,
      "learning_rate": 4.819155349101096e-05,
      "loss": 0.6848,
      "step": 5600
    },
    {
      "epoch": 0.20140631073106957,
      "grad_norm": 13.064956665039062,
      "learning_rate": 4.815609375554059e-05,
      "loss": 0.6934,
      "step": 5700
    },
    {
      "epoch": 0.20493975477898307,
      "grad_norm": 1.5064607858657837,
      "learning_rate": 4.812063402007021e-05,
      "loss": 0.6777,
      "step": 5800
    },
    {
      "epoch": 0.20847319882689658,
      "grad_norm": 6.365231037139893,
      "learning_rate": 4.808517428459984e-05,
      "loss": 0.672,
      "step": 5900
    },
    {
      "epoch": 0.21200664287481008,
      "grad_norm": 4.481011867523193,
      "learning_rate": 4.804971454912947e-05,
      "loss": 0.6874,
      "step": 6000
    },
    {
      "epoch": 0.21554008692272358,
      "grad_norm": 9.971890449523926,
      "learning_rate": 4.801425481365909e-05,
      "loss": 0.679,
      "step": 6100
    },
    {
      "epoch": 0.2190735309706371,
      "grad_norm": 3.795100450515747,
      "learning_rate": 4.797879507818872e-05,
      "loss": 0.6702,
      "step": 6200
    },
    {
      "epoch": 0.2226069750185506,
      "grad_norm": 5.969930171966553,
      "learning_rate": 4.794333534271834e-05,
      "loss": 0.6529,
      "step": 6300
    },
    {
      "epoch": 0.2261404190664641,
      "grad_norm": 16.5958251953125,
      "learning_rate": 4.790787560724797e-05,
      "loss": 0.6804,
      "step": 6400
    },
    {
      "epoch": 0.2296738631143776,
      "grad_norm": 11.099478721618652,
      "learning_rate": 4.78724158717776e-05,
      "loss": 0.6636,
      "step": 6500
    },
    {
      "epoch": 0.23320730716229107,
      "grad_norm": 6.668623447418213,
      "learning_rate": 4.783695613630723e-05,
      "loss": 0.6811,
      "step": 6600
    },
    {
      "epoch": 0.23674075121020458,
      "grad_norm": 1.8824816942214966,
      "learning_rate": 4.780149640083686e-05,
      "loss": 0.6707,
      "step": 6700
    },
    {
      "epoch": 0.24027419525811808,
      "grad_norm": 3.7134909629821777,
      "learning_rate": 4.776603666536648e-05,
      "loss": 0.6877,
      "step": 6800
    },
    {
      "epoch": 0.24380763930603158,
      "grad_norm": 8.107278823852539,
      "learning_rate": 4.77305769298961e-05,
      "loss": 0.6661,
      "step": 6900
    },
    {
      "epoch": 0.24734108335394508,
      "grad_norm": 5.763003349304199,
      "learning_rate": 4.769511719442573e-05,
      "loss": 0.6631,
      "step": 7000
    },
    {
      "epoch": 0.2508745274018586,
      "grad_norm": 10.514540672302246,
      "learning_rate": 4.765965745895536e-05,
      "loss": 0.68,
      "step": 7100
    },
    {
      "epoch": 0.2544079714497721,
      "grad_norm": 1.9705021381378174,
      "learning_rate": 4.762419772348499e-05,
      "loss": 0.6695,
      "step": 7200
    },
    {
      "epoch": 0.25794141549768557,
      "grad_norm": 24.187896728515625,
      "learning_rate": 4.758873798801461e-05,
      "loss": 0.6761,
      "step": 7300
    },
    {
      "epoch": 0.26147485954559907,
      "grad_norm": 13.003024101257324,
      "learning_rate": 4.755327825254424e-05,
      "loss": 0.6645,
      "step": 7400
    },
    {
      "epoch": 0.2650083035935126,
      "grad_norm": 4.125014781951904,
      "learning_rate": 4.751781851707386e-05,
      "loss": 0.6621,
      "step": 7500
    },
    {
      "epoch": 0.2685417476414261,
      "grad_norm": 2.8470189571380615,
      "learning_rate": 4.748235878160349e-05,
      "loss": 0.6502,
      "step": 7600
    },
    {
      "epoch": 0.2720751916893396,
      "grad_norm": 11.032081604003906,
      "learning_rate": 4.744689904613312e-05,
      "loss": 0.6707,
      "step": 7700
    },
    {
      "epoch": 0.2756086357372531,
      "grad_norm": 8.526495933532715,
      "learning_rate": 4.741143931066275e-05,
      "loss": 0.6639,
      "step": 7800
    },
    {
      "epoch": 0.2791420797851666,
      "grad_norm": 24.293764114379883,
      "learning_rate": 4.737597957519237e-05,
      "loss": 0.6747,
      "step": 7900
    },
    {
      "epoch": 0.2826755238330801,
      "grad_norm": 5.254375457763672,
      "learning_rate": 4.7340519839722e-05,
      "loss": 0.6867,
      "step": 8000
    },
    {
      "epoch": 0.2862089678809936,
      "grad_norm": 1.5120165348052979,
      "learning_rate": 4.730506010425162e-05,
      "loss": 0.677,
      "step": 8100
    },
    {
      "epoch": 0.2897424119289071,
      "grad_norm": 38.41202163696289,
      "learning_rate": 4.726960036878125e-05,
      "loss": 0.6805,
      "step": 8200
    },
    {
      "epoch": 0.2932758559768206,
      "grad_norm": 18.799192428588867,
      "learning_rate": 4.723414063331088e-05,
      "loss": 0.6566,
      "step": 8300
    },
    {
      "epoch": 0.2968093000247341,
      "grad_norm": 6.457893371582031,
      "learning_rate": 4.71986808978405e-05,
      "loss": 0.6657,
      "step": 8400
    },
    {
      "epoch": 0.3003427440726476,
      "grad_norm": 12.490777015686035,
      "learning_rate": 4.716322116237013e-05,
      "loss": 0.6912,
      "step": 8500
    },
    {
      "epoch": 0.3038761881205611,
      "grad_norm": 3.9266059398651123,
      "learning_rate": 4.712776142689976e-05,
      "loss": 0.665,
      "step": 8600
    },
    {
      "epoch": 0.3074096321684746,
      "grad_norm": 1.8936899900436401,
      "learning_rate": 4.709230169142939e-05,
      "loss": 0.6635,
      "step": 8700
    },
    {
      "epoch": 0.3109430762163881,
      "grad_norm": 5.1170125007629395,
      "learning_rate": 4.705684195595901e-05,
      "loss": 0.6727,
      "step": 8800
    },
    {
      "epoch": 0.3144765202643016,
      "grad_norm": 15.984655380249023,
      "learning_rate": 4.7021382220488633e-05,
      "loss": 0.6654,
      "step": 8900
    },
    {
      "epoch": 0.3180099643122151,
      "grad_norm": 2.389951467514038,
      "learning_rate": 4.698592248501826e-05,
      "loss": 0.6699,
      "step": 9000
    },
    {
      "epoch": 0.3215434083601286,
      "grad_norm": 18.307100296020508,
      "learning_rate": 4.695046274954789e-05,
      "loss": 0.6656,
      "step": 9100
    },
    {
      "epoch": 0.3250768524080421,
      "grad_norm": 3.8851442337036133,
      "learning_rate": 4.691500301407752e-05,
      "loss": 0.6555,
      "step": 9200
    },
    {
      "epoch": 0.32861029645595563,
      "grad_norm": 1.3034247159957886,
      "learning_rate": 4.687954327860715e-05,
      "loss": 0.6609,
      "step": 9300
    },
    {
      "epoch": 0.33214374050386913,
      "grad_norm": 9.693036079406738,
      "learning_rate": 4.684408354313677e-05,
      "loss": 0.6624,
      "step": 9400
    },
    {
      "epoch": 0.33567718455178264,
      "grad_norm": 2.3803322315216064,
      "learning_rate": 4.6808623807666394e-05,
      "loss": 0.6617,
      "step": 9500
    },
    {
      "epoch": 0.33921062859969614,
      "grad_norm": 2.7378697395324707,
      "learning_rate": 4.677316407219602e-05,
      "loss": 0.6389,
      "step": 9600
    },
    {
      "epoch": 0.34274407264760964,
      "grad_norm": 22.62300682067871,
      "learning_rate": 4.673770433672565e-05,
      "loss": 0.6618,
      "step": 9700
    },
    {
      "epoch": 0.34627751669552315,
      "grad_norm": 14.297917366027832,
      "learning_rate": 4.670224460125528e-05,
      "loss": 0.6651,
      "step": 9800
    },
    {
      "epoch": 0.34981096074343665,
      "grad_norm": 5.60073184967041,
      "learning_rate": 4.66667848657849e-05,
      "loss": 0.6582,
      "step": 9900
    },
    {
      "epoch": 0.35334440479135015,
      "grad_norm": 5.282256603240967,
      "learning_rate": 4.663132513031453e-05,
      "loss": 0.6705,
      "step": 10000
    },
    {
      "epoch": 0.35687784883926366,
      "grad_norm": 11.657859802246094,
      "learning_rate": 4.6595865394844154e-05,
      "loss": 0.6594,
      "step": 10100
    },
    {
      "epoch": 0.36041129288717716,
      "grad_norm": 9.610794067382812,
      "learning_rate": 4.656040565937378e-05,
      "loss": 0.6462,
      "step": 10200
    },
    {
      "epoch": 0.3639447369350906,
      "grad_norm": 10.929064750671387,
      "learning_rate": 4.652494592390341e-05,
      "loss": 0.688,
      "step": 10300
    },
    {
      "epoch": 0.3674781809830041,
      "grad_norm": 2.0145246982574463,
      "learning_rate": 4.6489486188433034e-05,
      "loss": 0.6674,
      "step": 10400
    },
    {
      "epoch": 0.3710116250309176,
      "grad_norm": 1.9821768999099731,
      "learning_rate": 4.645402645296266e-05,
      "loss": 0.656,
      "step": 10500
    },
    {
      "epoch": 0.3745450690788311,
      "grad_norm": 16.196889877319336,
      "learning_rate": 4.641856671749229e-05,
      "loss": 0.6629,
      "step": 10600
    },
    {
      "epoch": 0.3780785131267446,
      "grad_norm": 7.497737884521484,
      "learning_rate": 4.6383106982021914e-05,
      "loss": 0.6458,
      "step": 10700
    },
    {
      "epoch": 0.3816119571746581,
      "grad_norm": 12.679838180541992,
      "learning_rate": 4.634764724655154e-05,
      "loss": 0.6534,
      "step": 10800
    },
    {
      "epoch": 0.3851454012225716,
      "grad_norm": 1.8995715379714966,
      "learning_rate": 4.6312187511081166e-05,
      "loss": 0.6686,
      "step": 10900
    },
    {
      "epoch": 0.38867884527048513,
      "grad_norm": 9.472040176391602,
      "learning_rate": 4.6276727775610795e-05,
      "loss": 0.6398,
      "step": 11000
    },
    {
      "epoch": 0.39221228931839863,
      "grad_norm": 4.048205852508545,
      "learning_rate": 4.6241268040140424e-05,
      "loss": 0.6881,
      "step": 11100
    },
    {
      "epoch": 0.39574573336631214,
      "grad_norm": 15.861153602600098,
      "learning_rate": 4.620580830467005e-05,
      "loss": 0.6637,
      "step": 11200
    },
    {
      "epoch": 0.39927917741422564,
      "grad_norm": 2.0795767307281494,
      "learning_rate": 4.617034856919968e-05,
      "loss": 0.6623,
      "step": 11300
    },
    {
      "epoch": 0.40281262146213914,
      "grad_norm": 11.499054908752441,
      "learning_rate": 4.6134888833729304e-05,
      "loss": 0.6622,
      "step": 11400
    },
    {
      "epoch": 0.40634606551005265,
      "grad_norm": 8.266446113586426,
      "learning_rate": 4.6099429098258926e-05,
      "loss": 0.6581,
      "step": 11500
    },
    {
      "epoch": 0.40987950955796615,
      "grad_norm": 5.818386077880859,
      "learning_rate": 4.6063969362788555e-05,
      "loss": 0.6788,
      "step": 11600
    },
    {
      "epoch": 0.41341295360587965,
      "grad_norm": 7.767907619476318,
      "learning_rate": 4.6028509627318184e-05,
      "loss": 0.6466,
      "step": 11700
    },
    {
      "epoch": 0.41694639765379315,
      "grad_norm": 10.241583824157715,
      "learning_rate": 4.599304989184781e-05,
      "loss": 0.6417,
      "step": 11800
    },
    {
      "epoch": 0.42047984170170666,
      "grad_norm": 6.80443000793457,
      "learning_rate": 4.5957590156377435e-05,
      "loss": 0.6544,
      "step": 11900
    },
    {
      "epoch": 0.42401328574962016,
      "grad_norm": 7.006357669830322,
      "learning_rate": 4.5922130420907064e-05,
      "loss": 0.643,
      "step": 12000
    },
    {
      "epoch": 0.42754672979753366,
      "grad_norm": 1.6756291389465332,
      "learning_rate": 4.5886670685436686e-05,
      "loss": 0.6549,
      "step": 12100
    },
    {
      "epoch": 0.43108017384544717,
      "grad_norm": 12.71094799041748,
      "learning_rate": 4.5851210949966315e-05,
      "loss": 0.6753,
      "step": 12200
    },
    {
      "epoch": 0.43461361789336067,
      "grad_norm": 4.934825897216797,
      "learning_rate": 4.5815751214495944e-05,
      "loss": 0.6499,
      "step": 12300
    },
    {
      "epoch": 0.4381470619412742,
      "grad_norm": 6.342639446258545,
      "learning_rate": 4.5780291479025566e-05,
      "loss": 0.657,
      "step": 12400
    },
    {
      "epoch": 0.4416805059891877,
      "grad_norm": 7.1975998878479,
      "learning_rate": 4.5744831743555195e-05,
      "loss": 0.6503,
      "step": 12500
    },
    {
      "epoch": 0.4452139500371012,
      "grad_norm": 20.371601104736328,
      "learning_rate": 4.5709372008084824e-05,
      "loss": 0.6608,
      "step": 12600
    },
    {
      "epoch": 0.4487473940850147,
      "grad_norm": 4.152439117431641,
      "learning_rate": 4.5673912272614447e-05,
      "loss": 0.6661,
      "step": 12700
    },
    {
      "epoch": 0.4522808381329282,
      "grad_norm": 22.725751876831055,
      "learning_rate": 4.5638452537144076e-05,
      "loss": 0.6646,
      "step": 12800
    },
    {
      "epoch": 0.4558142821808417,
      "grad_norm": 13.746625900268555,
      "learning_rate": 4.56029928016737e-05,
      "loss": 0.6466,
      "step": 12900
    },
    {
      "epoch": 0.4593477262287552,
      "grad_norm": 30.028608322143555,
      "learning_rate": 4.556753306620333e-05,
      "loss": 0.6532,
      "step": 13000
    },
    {
      "epoch": 0.4628811702766687,
      "grad_norm": 4.501127243041992,
      "learning_rate": 4.5532073330732956e-05,
      "loss": 0.6524,
      "step": 13100
    },
    {
      "epoch": 0.46641461432458214,
      "grad_norm": 21.323190689086914,
      "learning_rate": 4.5496613595262585e-05,
      "loss": 0.6753,
      "step": 13200
    },
    {
      "epoch": 0.46994805837249565,
      "grad_norm": 20.27256202697754,
      "learning_rate": 4.5461153859792214e-05,
      "loss": 0.6629,
      "step": 13300
    },
    {
      "epoch": 0.47348150242040915,
      "grad_norm": 17.305938720703125,
      "learning_rate": 4.5425694124321836e-05,
      "loss": 0.6525,
      "step": 13400
    },
    {
      "epoch": 0.47701494646832265,
      "grad_norm": 5.578602313995361,
      "learning_rate": 4.539023438885146e-05,
      "loss": 0.6641,
      "step": 13500
    },
    {
      "epoch": 0.48054839051623616,
      "grad_norm": 9.811827659606934,
      "learning_rate": 4.535477465338109e-05,
      "loss": 0.6375,
      "step": 13600
    },
    {
      "epoch": 0.48408183456414966,
      "grad_norm": 3.6102564334869385,
      "learning_rate": 4.5319314917910716e-05,
      "loss": 0.6726,
      "step": 13700
    },
    {
      "epoch": 0.48761527861206316,
      "grad_norm": 5.063249588012695,
      "learning_rate": 4.5283855182440345e-05,
      "loss": 0.6543,
      "step": 13800
    },
    {
      "epoch": 0.49114872265997667,
      "grad_norm": 10.13953685760498,
      "learning_rate": 4.524839544696997e-05,
      "loss": 0.652,
      "step": 13900
    },
    {
      "epoch": 0.49468216670789017,
      "grad_norm": 2.995501756668091,
      "learning_rate": 4.5212935711499596e-05,
      "loss": 0.6475,
      "step": 14000
    },
    {
      "epoch": 0.4982156107558037,
      "grad_norm": 2.4435553550720215,
      "learning_rate": 4.517747597602922e-05,
      "loss": 0.6681,
      "step": 14100
    },
    {
      "epoch": 0.5017490548037172,
      "grad_norm": 3.2824742794036865,
      "learning_rate": 4.514201624055885e-05,
      "loss": 0.6628,
      "step": 14200
    },
    {
      "epoch": 0.5052824988516307,
      "grad_norm": 7.978828430175781,
      "learning_rate": 4.5106556505088476e-05,
      "loss": 0.6688,
      "step": 14300
    },
    {
      "epoch": 0.5088159428995442,
      "grad_norm": 4.943182468414307,
      "learning_rate": 4.50710967696181e-05,
      "loss": 0.6503,
      "step": 14400
    },
    {
      "epoch": 0.5123493869474577,
      "grad_norm": 12.832077980041504,
      "learning_rate": 4.503563703414773e-05,
      "loss": 0.6438,
      "step": 14500
    },
    {
      "epoch": 0.5158828309953711,
      "grad_norm": 3.9387245178222656,
      "learning_rate": 4.5000177298677356e-05,
      "loss": 0.6512,
      "step": 14600
    },
    {
      "epoch": 0.5194162750432847,
      "grad_norm": 10.25678825378418,
      "learning_rate": 4.496471756320698e-05,
      "loss": 0.6489,
      "step": 14700
    },
    {
      "epoch": 0.5229497190911981,
      "grad_norm": 8.686861038208008,
      "learning_rate": 4.492925782773661e-05,
      "loss": 0.6456,
      "step": 14800
    },
    {
      "epoch": 0.5264831631391117,
      "grad_norm": 3.837502956390381,
      "learning_rate": 4.489379809226623e-05,
      "loss": 0.6477,
      "step": 14900
    },
    {
      "epoch": 0.5300166071870251,
      "grad_norm": 5.550582408905029,
      "learning_rate": 4.485833835679586e-05,
      "loss": 0.6613,
      "step": 15000
    },
    {
      "epoch": 0.5335500512349387,
      "grad_norm": 2.181652545928955,
      "learning_rate": 4.482287862132549e-05,
      "loss": 0.644,
      "step": 15100
    },
    {
      "epoch": 0.5370834952828522,
      "grad_norm": 3.2538418769836426,
      "learning_rate": 4.478741888585512e-05,
      "loss": 0.6637,
      "step": 15200
    },
    {
      "epoch": 0.5406169393307657,
      "grad_norm": 8.780191421508789,
      "learning_rate": 4.4751959150384746e-05,
      "loss": 0.6472,
      "step": 15300
    },
    {
      "epoch": 0.5441503833786792,
      "grad_norm": 8.478846549987793,
      "learning_rate": 4.471649941491436e-05,
      "loss": 0.6654,
      "step": 15400
    },
    {
      "epoch": 0.5476838274265927,
      "grad_norm": 21.26091766357422,
      "learning_rate": 4.468103967944399e-05,
      "loss": 0.6757,
      "step": 15500
    },
    {
      "epoch": 0.5512172714745062,
      "grad_norm": 7.112481117248535,
      "learning_rate": 4.464557994397362e-05,
      "loss": 0.6597,
      "step": 15600
    },
    {
      "epoch": 0.5547507155224197,
      "grad_norm": 11.814697265625,
      "learning_rate": 4.461012020850325e-05,
      "loss": 0.6571,
      "step": 15700
    },
    {
      "epoch": 0.5582841595703332,
      "grad_norm": 8.395440101623535,
      "learning_rate": 4.457466047303288e-05,
      "loss": 0.6533,
      "step": 15800
    },
    {
      "epoch": 0.5618176036182467,
      "grad_norm": 1.607149362564087,
      "learning_rate": 4.45392007375625e-05,
      "loss": 0.6548,
      "step": 15900
    },
    {
      "epoch": 0.5653510476661602,
      "grad_norm": 6.1191558837890625,
      "learning_rate": 4.450374100209213e-05,
      "loss": 0.6451,
      "step": 16000
    },
    {
      "epoch": 0.5688844917140737,
      "grad_norm": 11.286828994750977,
      "learning_rate": 4.446828126662175e-05,
      "loss": 0.6573,
      "step": 16100
    },
    {
      "epoch": 0.5724179357619872,
      "grad_norm": 6.485701560974121,
      "learning_rate": 4.443282153115138e-05,
      "loss": 0.6625,
      "step": 16200
    },
    {
      "epoch": 0.5759513798099007,
      "grad_norm": 10.301690101623535,
      "learning_rate": 4.439736179568101e-05,
      "loss": 0.6548,
      "step": 16300
    },
    {
      "epoch": 0.5794848238578142,
      "grad_norm": 8.642794609069824,
      "learning_rate": 4.436190206021063e-05,
      "loss": 0.6489,
      "step": 16400
    },
    {
      "epoch": 0.5830182679057277,
      "grad_norm": 4.649158000946045,
      "learning_rate": 4.432644232474026e-05,
      "loss": 0.6552,
      "step": 16500
    },
    {
      "epoch": 0.5865517119536412,
      "grad_norm": 12.284675598144531,
      "learning_rate": 4.429098258926989e-05,
      "loss": 0.6419,
      "step": 16600
    },
    {
      "epoch": 0.5900851560015548,
      "grad_norm": 2.738813877105713,
      "learning_rate": 4.425552285379951e-05,
      "loss": 0.6517,
      "step": 16700
    },
    {
      "epoch": 0.5936186000494682,
      "grad_norm": 5.321156978607178,
      "learning_rate": 4.422006311832914e-05,
      "loss": 0.6617,
      "step": 16800
    },
    {
      "epoch": 0.5971520440973818,
      "grad_norm": 7.863699913024902,
      "learning_rate": 4.418460338285876e-05,
      "loss": 0.6562,
      "step": 16900
    },
    {
      "epoch": 0.6006854881452952,
      "grad_norm": 17.82103157043457,
      "learning_rate": 4.414914364738839e-05,
      "loss": 0.6533,
      "step": 17000
    },
    {
      "epoch": 0.6042189321932088,
      "grad_norm": 1.9267044067382812,
      "learning_rate": 4.411368391191802e-05,
      "loss": 0.6587,
      "step": 17100
    },
    {
      "epoch": 0.6077523762411222,
      "grad_norm": 8.837924003601074,
      "learning_rate": 4.407822417644765e-05,
      "loss": 0.6433,
      "step": 17200
    },
    {
      "epoch": 0.6112858202890358,
      "grad_norm": 2.6910996437072754,
      "learning_rate": 4.404276444097727e-05,
      "loss": 0.6691,
      "step": 17300
    },
    {
      "epoch": 0.6148192643369492,
      "grad_norm": 6.686689853668213,
      "learning_rate": 4.400730470550689e-05,
      "loss": 0.6391,
      "step": 17400
    },
    {
      "epoch": 0.6183527083848627,
      "grad_norm": 7.507197380065918,
      "learning_rate": 4.397184497003652e-05,
      "loss": 0.6552,
      "step": 17500
    },
    {
      "epoch": 0.6218861524327762,
      "grad_norm": 3.7144057750701904,
      "learning_rate": 4.393638523456615e-05,
      "loss": 0.6512,
      "step": 17600
    },
    {
      "epoch": 0.6254195964806897,
      "grad_norm": 3.3532116413116455,
      "learning_rate": 4.390092549909578e-05,
      "loss": 0.6651,
      "step": 17700
    },
    {
      "epoch": 0.6289530405286032,
      "grad_norm": 6.540914058685303,
      "learning_rate": 4.386546576362541e-05,
      "loss": 0.6537,
      "step": 17800
    },
    {
      "epoch": 0.6324864845765167,
      "grad_norm": 4.096885681152344,
      "learning_rate": 4.383000602815503e-05,
      "loss": 0.6536,
      "step": 17900
    },
    {
      "epoch": 0.6360199286244302,
      "grad_norm": 4.601438999176025,
      "learning_rate": 4.379454629268466e-05,
      "loss": 0.6655,
      "step": 18000
    },
    {
      "epoch": 0.6395533726723437,
      "grad_norm": 5.706069469451904,
      "learning_rate": 4.375908655721428e-05,
      "loss": 0.6536,
      "step": 18100
    },
    {
      "epoch": 0.6430868167202572,
      "grad_norm": 6.385014057159424,
      "learning_rate": 4.372362682174391e-05,
      "loss": 0.6666,
      "step": 18200
    },
    {
      "epoch": 0.6466202607681707,
      "grad_norm": 6.3510518074035645,
      "learning_rate": 4.368816708627354e-05,
      "loss": 0.6499,
      "step": 18300
    },
    {
      "epoch": 0.6501537048160843,
      "grad_norm": 1.3304264545440674,
      "learning_rate": 4.365270735080316e-05,
      "loss": 0.6507,
      "step": 18400
    },
    {
      "epoch": 0.6536871488639977,
      "grad_norm": 7.9272074699401855,
      "learning_rate": 4.361724761533279e-05,
      "loss": 0.6481,
      "step": 18500
    },
    {
      "epoch": 0.6572205929119113,
      "grad_norm": 5.276841163635254,
      "learning_rate": 4.358178787986242e-05,
      "loss": 0.6399,
      "step": 18600
    },
    {
      "epoch": 0.6607540369598247,
      "grad_norm": 15.917122840881348,
      "learning_rate": 4.354632814439204e-05,
      "loss": 0.6531,
      "step": 18700
    },
    {
      "epoch": 0.6642874810077383,
      "grad_norm": 3.5866854190826416,
      "learning_rate": 4.351086840892167e-05,
      "loss": 0.6584,
      "step": 18800
    },
    {
      "epoch": 0.6678209250556517,
      "grad_norm": 20.185441970825195,
      "learning_rate": 4.3475408673451294e-05,
      "loss": 0.6521,
      "step": 18900
    },
    {
      "epoch": 0.6713543691035653,
      "grad_norm": 5.895605564117432,
      "learning_rate": 4.343994893798092e-05,
      "loss": 0.6617,
      "step": 19000
    },
    {
      "epoch": 0.6748878131514787,
      "grad_norm": 10.968804359436035,
      "learning_rate": 4.340448920251055e-05,
      "loss": 0.6484,
      "step": 19100
    },
    {
      "epoch": 0.6784212571993923,
      "grad_norm": 13.333949089050293,
      "learning_rate": 4.336902946704018e-05,
      "loss": 0.6435,
      "step": 19200
    },
    {
      "epoch": 0.6819547012473057,
      "grad_norm": 15.534661293029785,
      "learning_rate": 4.33335697315698e-05,
      "loss": 0.6489,
      "step": 19300
    },
    {
      "epoch": 0.6854881452952193,
      "grad_norm": 1.5588672161102295,
      "learning_rate": 4.3298109996099426e-05,
      "loss": 0.6615,
      "step": 19400
    },
    {
      "epoch": 0.6890215893431327,
      "grad_norm": 2.523339033126831,
      "learning_rate": 4.3262650260629055e-05,
      "loss": 0.6459,
      "step": 19500
    },
    {
      "epoch": 0.6925550333910463,
      "grad_norm": 12.65608024597168,
      "learning_rate": 4.3227190525158683e-05,
      "loss": 0.6472,
      "step": 19600
    },
    {
      "epoch": 0.6960884774389597,
      "grad_norm": 9.171727180480957,
      "learning_rate": 4.319173078968831e-05,
      "loss": 0.636,
      "step": 19700
    },
    {
      "epoch": 0.6996219214868733,
      "grad_norm": 17.21831512451172,
      "learning_rate": 4.315627105421794e-05,
      "loss": 0.6432,
      "step": 19800
    },
    {
      "epoch": 0.7031553655347867,
      "grad_norm": 4.002025604248047,
      "learning_rate": 4.3120811318747564e-05,
      "loss": 0.6301,
      "step": 19900
    },
    {
      "epoch": 0.7066888095827003,
      "grad_norm": 13.168841361999512,
      "learning_rate": 4.3085351583277186e-05,
      "loss": 0.665,
      "step": 20000
    },
    {
      "epoch": 0.7102222536306138,
      "grad_norm": 9.28709602355957,
      "learning_rate": 4.3049891847806815e-05,
      "loss": 0.6594,
      "step": 20100
    },
    {
      "epoch": 0.7137556976785273,
      "grad_norm": 1.7805536985397339,
      "learning_rate": 4.3014432112336444e-05,
      "loss": 0.6422,
      "step": 20200
    },
    {
      "epoch": 0.7172891417264408,
      "grad_norm": 3.1696324348449707,
      "learning_rate": 4.297897237686607e-05,
      "loss": 0.6476,
      "step": 20300
    },
    {
      "epoch": 0.7208225857743543,
      "grad_norm": 12.28231143951416,
      "learning_rate": 4.2943512641395695e-05,
      "loss": 0.6515,
      "step": 20400
    },
    {
      "epoch": 0.7243560298222678,
      "grad_norm": 18.175498962402344,
      "learning_rate": 4.2908052905925324e-05,
      "loss": 0.6603,
      "step": 20500
    },
    {
      "epoch": 0.7278894738701812,
      "grad_norm": 5.273600101470947,
      "learning_rate": 4.287259317045495e-05,
      "loss": 0.6537,
      "step": 20600
    },
    {
      "epoch": 0.7314229179180948,
      "grad_norm": 14.877324104309082,
      "learning_rate": 4.2837133434984575e-05,
      "loss": 0.656,
      "step": 20700
    },
    {
      "epoch": 0.7349563619660082,
      "grad_norm": 8.08657455444336,
      "learning_rate": 4.2801673699514204e-05,
      "loss": 0.6381,
      "step": 20800
    },
    {
      "epoch": 0.7384898060139218,
      "grad_norm": 8.971698760986328,
      "learning_rate": 4.2766213964043826e-05,
      "loss": 0.6696,
      "step": 20900
    },
    {
      "epoch": 0.7420232500618352,
      "grad_norm": 2.715527057647705,
      "learning_rate": 4.2730754228573455e-05,
      "loss": 0.6493,
      "step": 21000
    },
    {
      "epoch": 0.7455566941097488,
      "grad_norm": 2.0340447425842285,
      "learning_rate": 4.2695294493103084e-05,
      "loss": 0.6343,
      "step": 21100
    },
    {
      "epoch": 0.7490901381576622,
      "grad_norm": 8.684538841247559,
      "learning_rate": 4.265983475763271e-05,
      "loss": 0.6586,
      "step": 21200
    },
    {
      "epoch": 0.7526235822055758,
      "grad_norm": 7.235558032989502,
      "learning_rate": 4.2624375022162335e-05,
      "loss": 0.6644,
      "step": 21300
    },
    {
      "epoch": 0.7561570262534892,
      "grad_norm": 12.81946849822998,
      "learning_rate": 4.2588915286691964e-05,
      "loss": 0.6501,
      "step": 21400
    },
    {
      "epoch": 0.7596904703014028,
      "grad_norm": 14.98034381866455,
      "learning_rate": 4.255345555122159e-05,
      "loss": 0.6477,
      "step": 21500
    },
    {
      "epoch": 0.7632239143493162,
      "grad_norm": 9.942084312438965,
      "learning_rate": 4.2517995815751216e-05,
      "loss": 0.6648,
      "step": 21600
    },
    {
      "epoch": 0.7667573583972298,
      "grad_norm": 1.6093205213546753,
      "learning_rate": 4.2482536080280845e-05,
      "loss": 0.6425,
      "step": 21700
    },
    {
      "epoch": 0.7702908024451433,
      "grad_norm": 11.306264877319336,
      "learning_rate": 4.2447076344810474e-05,
      "loss": 0.6665,
      "step": 21800
    },
    {
      "epoch": 0.7738242464930568,
      "grad_norm": 1.9293485879898071,
      "learning_rate": 4.2411616609340096e-05,
      "loss": 0.6434,
      "step": 21900
    },
    {
      "epoch": 0.7773576905409703,
      "grad_norm": 11.030557632446289,
      "learning_rate": 4.237615687386972e-05,
      "loss": 0.646,
      "step": 22000
    },
    {
      "epoch": 0.7808911345888838,
      "grad_norm": 4.33185338973999,
      "learning_rate": 4.234069713839935e-05,
      "loss": 0.6392,
      "step": 22100
    },
    {
      "epoch": 0.7844245786367973,
      "grad_norm": 15.403934478759766,
      "learning_rate": 4.2305237402928976e-05,
      "loss": 0.643,
      "step": 22200
    },
    {
      "epoch": 0.7879580226847108,
      "grad_norm": 10.676435470581055,
      "learning_rate": 4.2269777667458605e-05,
      "loss": 0.6553,
      "step": 22300
    },
    {
      "epoch": 0.7914914667326243,
      "grad_norm": 1.64539635181427,
      "learning_rate": 4.2234317931988234e-05,
      "loss": 0.6486,
      "step": 22400
    },
    {
      "epoch": 0.7950249107805378,
      "grad_norm": 4.324126243591309,
      "learning_rate": 4.2198858196517856e-05,
      "loss": 0.6543,
      "step": 22500
    },
    {
      "epoch": 0.7985583548284513,
      "grad_norm": 17.77163314819336,
      "learning_rate": 4.2163398461047485e-05,
      "loss": 0.6488,
      "step": 22600
    },
    {
      "epoch": 0.8020917988763648,
      "grad_norm": 3.4614150524139404,
      "learning_rate": 4.212793872557711e-05,
      "loss": 0.648,
      "step": 22700
    },
    {
      "epoch": 0.8056252429242783,
      "grad_norm": 7.901573657989502,
      "learning_rate": 4.2092478990106736e-05,
      "loss": 0.6413,
      "step": 22800
    },
    {
      "epoch": 0.8091586869721918,
      "grad_norm": 11.688419342041016,
      "learning_rate": 4.2057019254636365e-05,
      "loss": 0.6549,
      "step": 22900
    },
    {
      "epoch": 0.8126921310201053,
      "grad_norm": 7.149116516113281,
      "learning_rate": 4.202155951916599e-05,
      "loss": 0.6427,
      "step": 23000
    },
    {
      "epoch": 0.8162255750680188,
      "grad_norm": 3.2359588146209717,
      "learning_rate": 4.1986099783695616e-05,
      "loss": 0.6544,
      "step": 23100
    },
    {
      "epoch": 0.8197590191159323,
      "grad_norm": 15.507210731506348,
      "learning_rate": 4.1950640048225245e-05,
      "loss": 0.6697,
      "step": 23200
    },
    {
      "epoch": 0.8232924631638459,
      "grad_norm": 1.7114611864089966,
      "learning_rate": 4.191518031275487e-05,
      "loss": 0.6498,
      "step": 23300
    },
    {
      "epoch": 0.8268259072117593,
      "grad_norm": 7.778118133544922,
      "learning_rate": 4.18797205772845e-05,
      "loss": 0.6346,
      "step": 23400
    },
    {
      "epoch": 0.8303593512596728,
      "grad_norm": 2.662620782852173,
      "learning_rate": 4.184426084181412e-05,
      "loss": 0.6514,
      "step": 23500
    },
    {
      "epoch": 0.8338927953075863,
      "grad_norm": 15.087020874023438,
      "learning_rate": 4.180880110634375e-05,
      "loss": 0.658,
      "step": 23600
    },
    {
      "epoch": 0.8374262393554998,
      "grad_norm": 5.806881427764893,
      "learning_rate": 4.177334137087338e-05,
      "loss": 0.6537,
      "step": 23700
    },
    {
      "epoch": 0.8409596834034133,
      "grad_norm": 2.8988897800445557,
      "learning_rate": 4.1737881635403006e-05,
      "loss": 0.6616,
      "step": 23800
    },
    {
      "epoch": 0.8444931274513268,
      "grad_norm": 1.774000883102417,
      "learning_rate": 4.170242189993263e-05,
      "loss": 0.6556,
      "step": 23900
    },
    {
      "epoch": 0.8480265714992403,
      "grad_norm": 4.222719669342041,
      "learning_rate": 4.166696216446225e-05,
      "loss": 0.653,
      "step": 24000
    },
    {
      "epoch": 0.8515600155471538,
      "grad_norm": 2.705298662185669,
      "learning_rate": 4.163150242899188e-05,
      "loss": 0.6586,
      "step": 24100
    },
    {
      "epoch": 0.8550934595950673,
      "grad_norm": 12.30130386352539,
      "learning_rate": 4.159604269352151e-05,
      "loss": 0.6312,
      "step": 24200
    },
    {
      "epoch": 0.8586269036429808,
      "grad_norm": 21.848403930664062,
      "learning_rate": 4.156058295805114e-05,
      "loss": 0.6417,
      "step": 24300
    },
    {
      "epoch": 0.8621603476908943,
      "grad_norm": 21.659908294677734,
      "learning_rate": 4.1525123222580766e-05,
      "loss": 0.66,
      "step": 24400
    },
    {
      "epoch": 0.8656937917388078,
      "grad_norm": 15.017741203308105,
      "learning_rate": 4.148966348711039e-05,
      "loss": 0.638,
      "step": 24500
    },
    {
      "epoch": 0.8692272357867213,
      "grad_norm": 12.194679260253906,
      "learning_rate": 4.145420375164002e-05,
      "loss": 0.638,
      "step": 24600
    },
    {
      "epoch": 0.8727606798346348,
      "grad_norm": 6.465003967285156,
      "learning_rate": 4.141874401616964e-05,
      "loss": 0.6492,
      "step": 24700
    },
    {
      "epoch": 0.8762941238825483,
      "grad_norm": 7.157544136047363,
      "learning_rate": 4.138328428069927e-05,
      "loss": 0.6386,
      "step": 24800
    },
    {
      "epoch": 0.8798275679304618,
      "grad_norm": 10.674407958984375,
      "learning_rate": 4.13478245452289e-05,
      "loss": 0.6502,
      "step": 24900
    },
    {
      "epoch": 0.8833610119783754,
      "grad_norm": 3.5773143768310547,
      "learning_rate": 4.131236480975852e-05,
      "loss": 0.6411,
      "step": 25000
    },
    {
      "epoch": 0.8868944560262888,
      "grad_norm": 1.7722481489181519,
      "learning_rate": 4.127690507428815e-05,
      "loss": 0.6257,
      "step": 25100
    },
    {
      "epoch": 0.8904279000742024,
      "grad_norm": 4.772821426391602,
      "learning_rate": 4.124144533881778e-05,
      "loss": 0.6321,
      "step": 25200
    },
    {
      "epoch": 0.8939613441221158,
      "grad_norm": 6.729219436645508,
      "learning_rate": 4.12059856033474e-05,
      "loss": 0.655,
      "step": 25300
    },
    {
      "epoch": 0.8974947881700294,
      "grad_norm": 1.5413055419921875,
      "learning_rate": 4.117052586787703e-05,
      "loss": 0.6518,
      "step": 25400
    },
    {
      "epoch": 0.9010282322179428,
      "grad_norm": 8.987818717956543,
      "learning_rate": 4.113506613240665e-05,
      "loss": 0.6375,
      "step": 25500
    },
    {
      "epoch": 0.9045616762658564,
      "grad_norm": 5.881483554840088,
      "learning_rate": 4.109960639693628e-05,
      "loss": 0.6313,
      "step": 25600
    },
    {
      "epoch": 0.9080951203137698,
      "grad_norm": 10.961000442504883,
      "learning_rate": 4.106414666146591e-05,
      "loss": 0.6398,
      "step": 25700
    },
    {
      "epoch": 0.9116285643616834,
      "grad_norm": 7.119207382202148,
      "learning_rate": 4.102868692599554e-05,
      "loss": 0.6228,
      "step": 25800
    },
    {
      "epoch": 0.9151620084095968,
      "grad_norm": 2.1590492725372314,
      "learning_rate": 4.099322719052516e-05,
      "loss": 0.6377,
      "step": 25900
    },
    {
      "epoch": 0.9186954524575104,
      "grad_norm": 3.34590482711792,
      "learning_rate": 4.095776745505478e-05,
      "loss": 0.6538,
      "step": 26000
    },
    {
      "epoch": 0.9222288965054238,
      "grad_norm": 21.566905975341797,
      "learning_rate": 4.092230771958441e-05,
      "loss": 0.6518,
      "step": 26100
    },
    {
      "epoch": 0.9257623405533374,
      "grad_norm": 4.535843372344971,
      "learning_rate": 4.088684798411404e-05,
      "loss": 0.6367,
      "step": 26200
    },
    {
      "epoch": 0.9292957846012508,
      "grad_norm": 1.4236254692077637,
      "learning_rate": 4.085138824864367e-05,
      "loss": 0.6509,
      "step": 26300
    },
    {
      "epoch": 0.9328292286491643,
      "grad_norm": 2.083080530166626,
      "learning_rate": 4.08159285131733e-05,
      "loss": 0.6542,
      "step": 26400
    },
    {
      "epoch": 0.9363626726970778,
      "grad_norm": 5.8222126960754395,
      "learning_rate": 4.078046877770292e-05,
      "loss": 0.6543,
      "step": 26500
    },
    {
      "epoch": 0.9398961167449913,
      "grad_norm": 11.975235939025879,
      "learning_rate": 4.074500904223254e-05,
      "loss": 0.6424,
      "step": 26600
    },
    {
      "epoch": 0.9434295607929049,
      "grad_norm": 11.538087844848633,
      "learning_rate": 4.070954930676217e-05,
      "loss": 0.6481,
      "step": 26700
    },
    {
      "epoch": 0.9469630048408183,
      "grad_norm": 10.164650917053223,
      "learning_rate": 4.06740895712918e-05,
      "loss": 0.653,
      "step": 26800
    },
    {
      "epoch": 0.9504964488887319,
      "grad_norm": 13.390176773071289,
      "learning_rate": 4.063862983582143e-05,
      "loss": 0.6483,
      "step": 26900
    },
    {
      "epoch": 0.9540298929366453,
      "grad_norm": 5.124284744262695,
      "learning_rate": 4.060317010035105e-05,
      "loss": 0.6649,
      "step": 27000
    },
    {
      "epoch": 0.9575633369845589,
      "grad_norm": 22.5499267578125,
      "learning_rate": 4.056771036488068e-05,
      "loss": 0.6499,
      "step": 27100
    },
    {
      "epoch": 0.9610967810324723,
      "grad_norm": 8.043667793273926,
      "learning_rate": 4.053225062941031e-05,
      "loss": 0.6458,
      "step": 27200
    },
    {
      "epoch": 0.9646302250803859,
      "grad_norm": 1.538108468055725,
      "learning_rate": 4.049679089393993e-05,
      "loss": 0.6435,
      "step": 27300
    },
    {
      "epoch": 0.9681636691282993,
      "grad_norm": 3.0426554679870605,
      "learning_rate": 4.046133115846956e-05,
      "loss": 0.648,
      "step": 27400
    },
    {
      "epoch": 0.9716971131762129,
      "grad_norm": 5.103086948394775,
      "learning_rate": 4.042587142299918e-05,
      "loss": 0.6465,
      "step": 27500
    },
    {
      "epoch": 0.9752305572241263,
      "grad_norm": 6.34366512298584,
      "learning_rate": 4.039041168752881e-05,
      "loss": 0.6645,
      "step": 27600
    },
    {
      "epoch": 0.9787640012720399,
      "grad_norm": 7.367029190063477,
      "learning_rate": 4.035495195205844e-05,
      "loss": 0.6475,
      "step": 27700
    },
    {
      "epoch": 0.9822974453199533,
      "grad_norm": 5.164797306060791,
      "learning_rate": 4.031949221658807e-05,
      "loss": 0.6574,
      "step": 27800
    },
    {
      "epoch": 0.9858308893678669,
      "grad_norm": 16.68526840209961,
      "learning_rate": 4.028403248111769e-05,
      "loss": 0.6423,
      "step": 27900
    },
    {
      "epoch": 0.9893643334157803,
      "grad_norm": 11.954339981079102,
      "learning_rate": 4.0248572745647314e-05,
      "loss": 0.645,
      "step": 28000
    },
    {
      "epoch": 0.9928977774636939,
      "grad_norm": 1.9340643882751465,
      "learning_rate": 4.0213113010176943e-05,
      "loss": 0.6555,
      "step": 28100
    },
    {
      "epoch": 0.9964312215116073,
      "grad_norm": 2.497185230255127,
      "learning_rate": 4.017765327470657e-05,
      "loss": 0.65,
      "step": 28200
    },
    {
      "epoch": 0.9999646655595209,
      "grad_norm": 16.926382064819336,
      "learning_rate": 4.01421935392362e-05,
      "loss": 0.6476,
      "step": 28300
    },
    {
      "epoch": 1.0034981096074345,
      "grad_norm": 2.0805704593658447,
      "learning_rate": 4.010673380376583e-05,
      "loss": 0.6461,
      "step": 28400
    },
    {
      "epoch": 1.0070315536553478,
      "grad_norm": 3.0861270427703857,
      "learning_rate": 4.007127406829545e-05,
      "loss": 0.6473,
      "step": 28500
    },
    {
      "epoch": 1.0105649977032614,
      "grad_norm": 1.852721929550171,
      "learning_rate": 4.0035814332825075e-05,
      "loss": 0.6334,
      "step": 28600
    },
    {
      "epoch": 1.014098441751175,
      "grad_norm": 6.178478240966797,
      "learning_rate": 4.0000354597354704e-05,
      "loss": 0.6509,
      "step": 28700
    },
    {
      "epoch": 1.0176318857990885,
      "grad_norm": 1.4755687713623047,
      "learning_rate": 3.996489486188433e-05,
      "loss": 0.6371,
      "step": 28800
    },
    {
      "epoch": 1.0211653298470018,
      "grad_norm": 14.55130386352539,
      "learning_rate": 3.992943512641396e-05,
      "loss": 0.6447,
      "step": 28900
    },
    {
      "epoch": 1.0246987738949154,
      "grad_norm": 9.035788536071777,
      "learning_rate": 3.9893975390943584e-05,
      "loss": 0.6455,
      "step": 29000
    },
    {
      "epoch": 1.028232217942829,
      "grad_norm": 19.627904891967773,
      "learning_rate": 3.985851565547321e-05,
      "loss": 0.6659,
      "step": 29100
    },
    {
      "epoch": 1.0317656619907423,
      "grad_norm": 4.486616134643555,
      "learning_rate": 3.982305592000284e-05,
      "loss": 0.6461,
      "step": 29200
    },
    {
      "epoch": 1.0352991060386558,
      "grad_norm": 21.096155166625977,
      "learning_rate": 3.9787596184532464e-05,
      "loss": 0.634,
      "step": 29300
    },
    {
      "epoch": 1.0388325500865694,
      "grad_norm": 7.546743392944336,
      "learning_rate": 3.975213644906209e-05,
      "loss": 0.633,
      "step": 29400
    },
    {
      "epoch": 1.042365994134483,
      "grad_norm": 5.000208854675293,
      "learning_rate": 3.9716676713591715e-05,
      "loss": 0.6285,
      "step": 29500
    },
    {
      "epoch": 1.0458994381823963,
      "grad_norm": 20.753456115722656,
      "learning_rate": 3.9681216978121344e-05,
      "loss": 0.6434,
      "step": 29600
    },
    {
      "epoch": 1.0494328822303098,
      "grad_norm": 16.756105422973633,
      "learning_rate": 3.964575724265097e-05,
      "loss": 0.6518,
      "step": 29700
    },
    {
      "epoch": 1.0529663262782234,
      "grad_norm": 19.914533615112305,
      "learning_rate": 3.96102975071806e-05,
      "loss": 0.6452,
      "step": 29800
    },
    {
      "epoch": 1.056499770326137,
      "grad_norm": 14.600440979003906,
      "learning_rate": 3.9574837771710224e-05,
      "loss": 0.6373,
      "step": 29900
    },
    {
      "epoch": 1.0600332143740503,
      "grad_norm": 15.438008308410645,
      "learning_rate": 3.953937803623985e-05,
      "loss": 0.6495,
      "step": 30000
    },
    {
      "epoch": 1.0635666584219639,
      "grad_norm": 1.5307095050811768,
      "learning_rate": 3.9503918300769476e-05,
      "loss": 0.6532,
      "step": 30100
    },
    {
      "epoch": 1.0671001024698774,
      "grad_norm": 8.342841148376465,
      "learning_rate": 3.9468458565299105e-05,
      "loss": 0.6441,
      "step": 30200
    },
    {
      "epoch": 1.070633546517791,
      "grad_norm": 2.2434470653533936,
      "learning_rate": 3.9432998829828734e-05,
      "loss": 0.6494,
      "step": 30300
    },
    {
      "epoch": 1.0741669905657043,
      "grad_norm": 1.8077353239059448,
      "learning_rate": 3.939753909435836e-05,
      "loss": 0.6479,
      "step": 30400
    },
    {
      "epoch": 1.0777004346136179,
      "grad_norm": 4.004906177520752,
      "learning_rate": 3.9362079358887985e-05,
      "loss": 0.6284,
      "step": 30500
    },
    {
      "epoch": 1.0812338786615314,
      "grad_norm": 3.9161195755004883,
      "learning_rate": 3.932661962341761e-05,
      "loss": 0.6375,
      "step": 30600
    },
    {
      "epoch": 1.084767322709445,
      "grad_norm": 14.511553764343262,
      "learning_rate": 3.9291159887947236e-05,
      "loss": 0.6376,
      "step": 30700
    },
    {
      "epoch": 1.0883007667573583,
      "grad_norm": 5.341761589050293,
      "learning_rate": 3.9255700152476865e-05,
      "loss": 0.6399,
      "step": 30800
    },
    {
      "epoch": 1.0918342108052719,
      "grad_norm": 1.6764591932296753,
      "learning_rate": 3.9220240417006494e-05,
      "loss": 0.6533,
      "step": 30900
    },
    {
      "epoch": 1.0953676548531854,
      "grad_norm": 12.866436004638672,
      "learning_rate": 3.9184780681536116e-05,
      "loss": 0.641,
      "step": 31000
    },
    {
      "epoch": 1.098901098901099,
      "grad_norm": 7.033936500549316,
      "learning_rate": 3.9149320946065745e-05,
      "loss": 0.6367,
      "step": 31100
    },
    {
      "epoch": 1.1024345429490123,
      "grad_norm": 1.810786247253418,
      "learning_rate": 3.911386121059537e-05,
      "loss": 0.6416,
      "step": 31200
    },
    {
      "epoch": 1.105967986996926,
      "grad_norm": 5.0479350090026855,
      "learning_rate": 3.9078401475124996e-05,
      "loss": 0.6472,
      "step": 31300
    },
    {
      "epoch": 1.1095014310448394,
      "grad_norm": 13.563732147216797,
      "learning_rate": 3.9042941739654625e-05,
      "loss": 0.6535,
      "step": 31400
    },
    {
      "epoch": 1.113034875092753,
      "grad_norm": 5.186655521392822,
      "learning_rate": 3.900748200418425e-05,
      "loss": 0.6631,
      "step": 31500
    },
    {
      "epoch": 1.1165683191406663,
      "grad_norm": 16.66447639465332,
      "learning_rate": 3.8972022268713876e-05,
      "loss": 0.6429,
      "step": 31600
    },
    {
      "epoch": 1.12010176318858,
      "grad_norm": 6.181554794311523,
      "learning_rate": 3.8936562533243505e-05,
      "loss": 0.6491,
      "step": 31700
    },
    {
      "epoch": 1.1236352072364935,
      "grad_norm": 4.848426342010498,
      "learning_rate": 3.8901102797773134e-05,
      "loss": 0.6276,
      "step": 31800
    },
    {
      "epoch": 1.127168651284407,
      "grad_norm": 1.9873825311660767,
      "learning_rate": 3.8865643062302757e-05,
      "loss": 0.6423,
      "step": 31900
    },
    {
      "epoch": 1.1307020953323204,
      "grad_norm": 5.610396862030029,
      "learning_rate": 3.883018332683238e-05,
      "loss": 0.6437,
      "step": 32000
    },
    {
      "epoch": 1.134235539380234,
      "grad_norm": 18.487319946289062,
      "learning_rate": 3.879472359136201e-05,
      "loss": 0.6397,
      "step": 32100
    },
    {
      "epoch": 1.1377689834281475,
      "grad_norm": 2.3182477951049805,
      "learning_rate": 3.875926385589164e-05,
      "loss": 0.6332,
      "step": 32200
    },
    {
      "epoch": 1.1413024274760608,
      "grad_norm": 9.156807899475098,
      "learning_rate": 3.8723804120421266e-05,
      "loss": 0.6314,
      "step": 32300
    },
    {
      "epoch": 1.1448358715239744,
      "grad_norm": 8.534097671508789,
      "learning_rate": 3.8688344384950895e-05,
      "loss": 0.6342,
      "step": 32400
    },
    {
      "epoch": 1.148369315571888,
      "grad_norm": 1.8613885641098022,
      "learning_rate": 3.865288464948052e-05,
      "loss": 0.6423,
      "step": 32500
    },
    {
      "epoch": 1.1519027596198015,
      "grad_norm": 12.589072227478027,
      "learning_rate": 3.861742491401014e-05,
      "loss": 0.6509,
      "step": 32600
    },
    {
      "epoch": 1.155436203667715,
      "grad_norm": 6.567150592803955,
      "learning_rate": 3.858196517853977e-05,
      "loss": 0.6345,
      "step": 32700
    },
    {
      "epoch": 1.1589696477156284,
      "grad_norm": 10.246580123901367,
      "learning_rate": 3.85465054430694e-05,
      "loss": 0.6314,
      "step": 32800
    },
    {
      "epoch": 1.162503091763542,
      "grad_norm": 7.370537757873535,
      "learning_rate": 3.8511045707599026e-05,
      "loss": 0.6532,
      "step": 32900
    },
    {
      "epoch": 1.1660365358114555,
      "grad_norm": 2.2042741775512695,
      "learning_rate": 3.847558597212865e-05,
      "loss": 0.6583,
      "step": 33000
    },
    {
      "epoch": 1.1695699798593688,
      "grad_norm": 12.264575004577637,
      "learning_rate": 3.844012623665828e-05,
      "loss": 0.6414,
      "step": 33100
    },
    {
      "epoch": 1.1731034239072824,
      "grad_norm": 7.935978889465332,
      "learning_rate": 3.84046665011879e-05,
      "loss": 0.6513,
      "step": 33200
    },
    {
      "epoch": 1.176636867955196,
      "grad_norm": 4.3838300704956055,
      "learning_rate": 3.836920676571753e-05,
      "loss": 0.639,
      "step": 33300
    },
    {
      "epoch": 1.1801703120031095,
      "grad_norm": 2.766982078552246,
      "learning_rate": 3.833374703024716e-05,
      "loss": 0.6597,
      "step": 33400
    },
    {
      "epoch": 1.1837037560510228,
      "grad_norm": 13.19021224975586,
      "learning_rate": 3.829828729477678e-05,
      "loss": 0.6417,
      "step": 33500
    },
    {
      "epoch": 1.1872372000989364,
      "grad_norm": 14.842268943786621,
      "learning_rate": 3.826282755930641e-05,
      "loss": 0.6517,
      "step": 33600
    },
    {
      "epoch": 1.19077064414685,
      "grad_norm": 14.827709197998047,
      "learning_rate": 3.822736782383604e-05,
      "loss": 0.646,
      "step": 33700
    },
    {
      "epoch": 1.1943040881947635,
      "grad_norm": 11.005945205688477,
      "learning_rate": 3.8191908088365667e-05,
      "loss": 0.6416,
      "step": 33800
    },
    {
      "epoch": 1.1978375322426769,
      "grad_norm": 3.229475259780884,
      "learning_rate": 3.815644835289529e-05,
      "loss": 0.6425,
      "step": 33900
    },
    {
      "epoch": 1.2013709762905904,
      "grad_norm": 5.011087894439697,
      "learning_rate": 3.812098861742491e-05,
      "loss": 0.6587,
      "step": 34000
    },
    {
      "epoch": 1.204904420338504,
      "grad_norm": 4.405858993530273,
      "learning_rate": 3.808552888195454e-05,
      "loss": 0.6432,
      "step": 34100
    },
    {
      "epoch": 1.2084378643864175,
      "grad_norm": 2.3271825313568115,
      "learning_rate": 3.805006914648417e-05,
      "loss": 0.6377,
      "step": 34200
    },
    {
      "epoch": 1.2119713084343309,
      "grad_norm": 4.211480617523193,
      "learning_rate": 3.80146094110138e-05,
      "loss": 0.6511,
      "step": 34300
    },
    {
      "epoch": 1.2155047524822444,
      "grad_norm": 5.6549153327941895,
      "learning_rate": 3.797914967554343e-05,
      "loss": 0.637,
      "step": 34400
    },
    {
      "epoch": 1.219038196530158,
      "grad_norm": 21.42514991760254,
      "learning_rate": 3.794368994007305e-05,
      "loss": 0.638,
      "step": 34500
    },
    {
      "epoch": 1.2225716405780713,
      "grad_norm": 2.299896001815796,
      "learning_rate": 3.790823020460267e-05,
      "loss": 0.6534,
      "step": 34600
    },
    {
      "epoch": 1.2261050846259849,
      "grad_norm": 5.4676642417907715,
      "learning_rate": 3.78727704691323e-05,
      "loss": 0.6398,
      "step": 34700
    },
    {
      "epoch": 1.2296385286738984,
      "grad_norm": 9.892848014831543,
      "learning_rate": 3.783731073366193e-05,
      "loss": 0.6612,
      "step": 34800
    },
    {
      "epoch": 1.233171972721812,
      "grad_norm": 2.1929473876953125,
      "learning_rate": 3.780185099819156e-05,
      "loss": 0.6288,
      "step": 34900
    },
    {
      "epoch": 1.2367054167697256,
      "grad_norm": 1.687846064567566,
      "learning_rate": 3.776639126272119e-05,
      "loss": 0.6396,
      "step": 35000
    }
  ],
  "logging_steps": 100,
  "max_steps": 141505,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 10,
  "trial_name": null,
  "trial_params": null
}
